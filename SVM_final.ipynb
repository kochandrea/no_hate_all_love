{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was run on an Amazon SageMaker ml.c5.4xlarge instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle_functions as pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_functions as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pf.read_pickle(bucket_name='advancedml-koch-mathur-hinkson', filename='sub_train_df14_preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['split', 'cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter', 'cleaned_lancaster', 'bigrams_unstemmed',\n",
    "       'perc_upper', 'num_exclam', 'num_words', 'perc_stopwords',\n",
    "       'num_upper_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 49)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count',\n",
       "       'cleaned_w_stopwords_str', 'cleaned_no_stem_str', 'cleaned_porter_str',\n",
       "       'cleaned_lancaster_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called \"toxicity_category\" in the train data frame categorizing comments as toxic (\"1\") or non-toxic (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into training (80%) and hold out sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "hold_out_set = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    74909\n",
      "1     5332\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    18435\n",
      "1     1324\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(hold_out_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    74909\n",
      "1     5332\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_set[train_set.toxicity_category == 1]\n",
    "nontoxic = train_set[train_set.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80241, 50), (5332, 50), (74909, 50))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the dataset to be include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = train_set.sample(quarter*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data set of 25% toxic and 75% nontoxic comments, and shuffle the data such that you do not have a data set of grouped toxic and grouped nontoxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15996\n",
      "1     5332\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_25 = toxic.append(nontoxic.sample(len(toxic)*3))\n",
    "prepared_25 = prepared_25.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_25.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - weighted 25% toxic and 75% nontoxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model now\n"
     ]
    }
   ],
   "source": [
    "classifier, output, fitted_vectorizer = mf.run_model(model_df=prepared_25, \n",
    "                                                     model_type=\"SVM\", \n",
    "                                                     comments = \"cleaned_no_stem_str\", \n",
    "                                                     train_perc=0.95, \n",
    "                                                     target=\"toxicity_category\", \n",
    "                                                     see_inside=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.875234521575985\n",
      "Overall Precision: 0.835\n",
      "Overall Recall: 0.6254681647940075\n",
      "Overall F1 Score: 0.715203426124197\n",
      "ROC_AUC: 0.792\n",
      "\n",
      "Target Accuracy: 0.6254681647940075\n",
      "Target Precision: 1.0\n",
      "Target Recall: 0.6254681647940075\n",
      "Target F1 Score: 0.7695852534562212\n",
      "\n",
      "Non-Target Accuracy: 0.9586983729662077\n",
      "Non-Target Precision: 1.0\n",
      "Non-Target Recall: 0.9586983729662077\n",
      "Non-Target F1 Score: 0.9789137380191694\n",
      "\n",
      "Strong Identity Accuracy: 0.45\n",
      "Strong Identity Precision: 1.0\n",
      "Strong Identity Recall: 0.45\n",
      "Strong Identity F1 Score: 1.0\n",
      "\n",
      "Obscenity Accuracy: 0.7272727272727273\n",
      "Obscenity Precision: 1.0\n",
      "Obscenity Recall: 0.7272727272727273\n",
      "Obscenity F1 Score: 1.0\n",
      "\n",
      "Insults Accuracy: 0.7128712871287128\n",
      "Insults Precision: 1.0\n",
      "Insults Recall: 0.7114427860696517\n",
      "Insults F1 Score: 1.0\n",
      "\n",
      "Threats Accuracy: 0.5\n",
      "Threats Precision: 1.0\n",
      "Threats Recall: 0.5\n",
      "Threats F1 Score: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Overall': {'Accuracy': 0.875234521575985,\n",
       "  'Precision': 0.835,\n",
       "  'Recall': 0.6254681647940075,\n",
       "  'F1': 0.715203426124197,\n",
       "  'ROC_AUC': 0.792},\n",
       " 'Target': {'Accuracy': 0.6254681647940075,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 0.6254681647940075,\n",
       "  'F1': 0.7695852534562212},\n",
       " 'Non-Target': {'Accuracy': 0.5, 'Precision': 1.0, 'Recall': 0.5, 'F1': 1.0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf.get_metrics(output=output, detailed=True, should_print=True, round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
      "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
      "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
      "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
      "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
      "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
      "       'other_sexual_orientation', 'physical_disability',\n",
      "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
      "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
      "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
      "       'identity_annotator_count', 'toxicity_annotator_count',\n",
      "       'cleaned_w_stopwords_str', 'cleaned_no_stem_str', 'cleaned_porter_str',\n",
      "       'cleaned_lancaster_str', 'toxicity_category', 'predicted', 'y_test'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hold_out_results = mf.run_model_test(model_df=hold_out_set, \n",
    "                                     clf=classifier, \n",
    "                                     vectorizer=fitted_vectorizer, \n",
    "                                     comments=\"cleaned_no_stem_str\", target=\"toxicity_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_results.to_csv(\"holdout_results\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9436712384229972\n",
      "Overall Precision: 0.5737246680642907\n",
      "Overall Recall: 0.6200906344410876\n",
      "Overall F1 Score: 0.5960072595281306\n",
      "ROC_AUC: 0.794\n",
      "\n",
      "Target Accuracy: 0.6200906344410876\n",
      "Target Precision: 1.0\n",
      "Target Recall: 0.6200906344410876\n",
      "Target F1 Score: 0.7655011655011655\n",
      "\n",
      "Non-Target Accuracy: 0.9669107675617032\n",
      "Non-Target Precision: 1.0\n",
      "Non-Target Recall: 0.9669107675617032\n",
      "Non-Target F1 Score: 0.983177054605626\n",
      "\n",
      "Strong Identity Accuracy: 0.5433070866141733\n",
      "Strong Identity Precision: 0.9565217391304348\n",
      "Strong Identity Recall: 0.5454545454545454\n",
      "Strong Identity F1 Score: 1.0\n",
      "\n",
      "Obscenity Accuracy: 0.7761194029850746\n",
      "Obscenity Precision: 1.0\n",
      "Obscenity Recall: 0.7761194029850746\n",
      "Obscenity F1 Score: 1.0\n",
      "\n",
      "Insults Accuracy: 0.6963562753036437\n",
      "Insults Precision: 0.9896296296296296\n",
      "Insults Recall: 0.6951092611862643\n",
      "Insults F1 Score: 1.0\n",
      "\n",
      "Threats Accuracy: 0.2413793103448276\n",
      "Threats Precision: 1.0\n",
      "Threats Recall: 0.21428571428571427\n",
      "Threats F1 Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hold_out_metrics = mf.get_metrics(output=hold_out_results, detailed=True, should_print=True, round_to=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - 1/3 toxic, 2/3 nontoxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10664\n",
      "1     5332\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_33 = toxic.append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_33 = prepared_33.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_33.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model now\n"
     ]
    }
   ],
   "source": [
    "classifier, output, fitted_vectorizer = mf.run_model(model_df=prepared_33, \n",
    "                                                     model_type=\"SVM\", \n",
    "                                                     comments = \"cleaned_no_stem_str\", \n",
    "                                                     train_perc=0.95, \n",
    "                                                     target=\"toxicity_category\", \n",
    "                                                     see_inside=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
      "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
      "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
      "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
      "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
      "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
      "       'other_sexual_orientation', 'physical_disability',\n",
      "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
      "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
      "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
      "       'identity_annotator_count', 'toxicity_annotator_count',\n",
      "       'cleaned_w_stopwords_str', 'cleaned_no_stem_str', 'cleaned_porter_str',\n",
      "       'cleaned_lancaster_str', 'toxicity_category', 'predicted', 'y_test'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hold_out_results = mf.run_model_test(model_df=hold_out_set, \n",
    "                                     clf=classifier, \n",
    "                                     vectorizer=fitted_vectorizer, \n",
    "                                     comments=\"cleaned_no_stem_str\", target=\"toxicity_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.9248949845639961\n",
      "Overall Precision: 0.45987963891675027\n",
      "Overall Recall: 0.6925981873111783\n",
      "Overall F1 Score: 0.5527426160337553\n",
      "ROC_AUC: 0.817\n",
      "\n",
      "Target Accuracy: 0.6925981873111783\n",
      "Target Precision: 1.0\n",
      "Target Recall: 0.6925981873111783\n",
      "Target F1 Score: 0.8183846497099508\n",
      "\n",
      "Non-Target Accuracy: 0.9415785191212368\n",
      "Non-Target Precision: 1.0\n",
      "Non-Target Recall: 0.9415785191212368\n",
      "Non-Target F1 Score: 0.9699103176598776\n",
      "\n",
      "Strong Identity Accuracy: 0.6535433070866141\n",
      "Strong Identity Precision: 0.9425287356321839\n",
      "Strong Identity Recall: 0.6776859504132231\n",
      "Strong Identity F1 Score: 1.0\n",
      "\n",
      "Obscenity Accuracy: 0.7611940298507462\n",
      "Obscenity Precision: 1.0\n",
      "Obscenity Recall: 0.7611940298507462\n",
      "Obscenity F1 Score: 1.0\n",
      "\n",
      "Insults Accuracy: 0.742914979757085\n",
      "Insults Precision: 0.9849108367626886\n",
      "Insults Recall: 0.7471383975026015\n",
      "Insults F1 Score: 1.0\n",
      "\n",
      "Threats Accuracy: 0.3793103448275862\n",
      "Threats Precision: 1.0\n",
      "Threats Recall: 0.35714285714285715\n",
      "Threats F1 Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hold_out_metrics = mf.get_metrics(output=hold_out_results, detailed=True, should_print=True, round_to=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
