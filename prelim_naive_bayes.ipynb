{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "import datetime\n",
    "\n",
    "import s3fs\n",
    "\n",
    "# import lch_proprietary.ml_modeling as ml \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into train (80%) and validation sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = train[:360000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train[360000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up process, create small samples of the train_set on which to run NB.  Ensure that samples are iid by replacing after each draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample1 = train_set.sample(frac=0.5, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722437, 45)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "DESTINATION = 'advancedml-koch-mathur-hinkson'\n",
    "\n",
    "def _write_dataframe_to_csv_on_s3(dataframe, filename):\n",
    "    \"\"\" Write a dataframe to a CSV on S3 \"\"\"\n",
    "    print(\"Writing {} records to {}\".format(len(dataframe), filename))\n",
    "    # Create buffer\n",
    "    csv_buffer = StringIO()\n",
    "    # Write dataframe to buffer\n",
    "    dataframe.to_csv(csv_buffer, sep=\"|\", index=False)\n",
    "    # Create S3 object\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    # Write buffer to S3 object\n",
    "    s3_resource.Object(DESTINATION, filename).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "_write_dataframe_to_csv_on_s3(train_sample1, 'preprocess_training_sample_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.add('')\n",
    "\n",
    "def clean_text(text, stemming=None, remove_sw = True):\n",
    "\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    \n",
    "    t = [w.lower() for w in t]\n",
    "    \n",
    "    if remove_sw == True:\n",
    "        t = [w for w in t if w not in sw]\n",
    "    \n",
    "    if stemming == None:\n",
    "        pass;\n",
    "    elif stemming == \"Porter\":\n",
    "        t = [ps.stem(w) for w in t]\n",
    "    elif stemming == \"Lancaster\":\n",
    "        t = [ls.stem(w) for w in t]\n",
    "    else:\n",
    "        print(\"Please enter a valid stemming type\")\n",
    "        \n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "\n",
    "\n",
    "    return ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_cleaning_cols(df):\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,False),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(\"Porter\",),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(\"Lancaster\",),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    \n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_stopwords'] = round((df.num_words - df['cleaned_no_stem'].apply(lambda x: len(x)))/df.num_words,3) \n",
    "    \n",
    "    df['num_upper_words'] = df[\"split\"].apply(lambda x: sum(map(str.isupper, x)) \n",
    ")\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25 21:19:49.786419\n",
      "2019-05-25 21:20:10.558744\n",
      "2019-05-25 21:29:42.878572\n",
      "2019-05-25 21:29:47.732161\n",
      "2019-05-25 21:29:52.538530\n"
     ]
    }
   ],
   "source": [
    "add_text_cleaning_cols(train_sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count', 'split',\n",
       "       'cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter',\n",
       "       'cleaned_lancaster', 'perc_upper', 'num_exclam', 'num_words',\n",
       "       'perc_stopwords', 'num_upper_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataset and send to s3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3D08FE279E1DFF4D',\n",
       "  'HostId': 'JlkdfeNjOxKkim8osIrvDNH4FosgJifyZrAjvnMeTVsuJEjqJVg6a40it0wYV65bbe00zWu6T5I=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'JlkdfeNjOxKkim8osIrvDNH4FosgJifyZrAjvnMeTVsuJEjqJVg6a40it0wYV65bbe00zWu6T5I=',\n",
       "   'x-amz-request-id': '3D08FE279E1DFF4D',\n",
       "   'date': 'Sat, 25 May 2019 23:05:55 GMT',\n",
       "   'etag': '\"74e5f9fe7a29990cbb2b86b7cc4296e0\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"74e5f9fe7a29990cbb2b86b7cc4296e0\"'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pickle_buffer = io.BytesIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket='advancedml-koch-mathur-hinkson'\n",
    "key='preprocessed_train_sample_50pct.pkl'\n",
    "\n",
    "# test.to_pickle(pickle_buffer)\n",
    "# s3_resource.Object(bucket, 'full_preprocessed_test.pkl').put(Body=pickle_buffer.getvalue())\n",
    "\n",
    "test.to_pickle(key)\n",
    "s3_resource.Object(bucket,key).put(Body=open(key, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtl_cols = ['perc_upper', 'num_exclam', 'num_words', 'perc_stopwords', 'num_upper_words']\n",
    "#addtl_cols = ['perc_upper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722437, 55)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>split</th>\n",
       "      <th>cleaned_w_stopwords</th>\n",
       "      <th>cleaned_no_stem</th>\n",
       "      <th>cleaned_porter</th>\n",
       "      <th>cleaned_lancaster</th>\n",
       "      <th>perc_upper</th>\n",
       "      <th>num_exclam</th>\n",
       "      <th>num_words</th>\n",
       "      <th>perc_stopwords</th>\n",
       "      <th>num_upper_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488037</th>\n",
       "      <td>842255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>What, you don't think the earth is at least 8,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[What,, you, don't, think, the, earth, is, at,...</td>\n",
       "      <td>what you don't think the earth is at least 8,0...</td>\n",
       "      <td>what think earth least 8,000 years old</td>\n",
       "      <td>what think earth least 8,000 year old</td>\n",
       "      <td>what think ear least 8,000 year old</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>-2.167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851755</th>\n",
       "      <td>5162644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IMO one reason is the \"degree of foresight\".  ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[IMO, one, reason, is, the, \"degree, of, fores...</td>\n",
       "      <td>imo one reason is the degree of foresight  tho...</td>\n",
       "      <td>imo one reason degree foresight less foresight...</td>\n",
       "      <td>imo one reason degre foresight less foresight ...</td>\n",
       "      <td>imo on reason degree foresight less foresight ...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>-1.660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830924</th>\n",
       "      <td>5137657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\" Oh- and throw in some more \"racist\" and \"mys...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[\", Oh-, and, throw, in, some, more, \"racist\",...</td>\n",
       "      <td>oh  and throw in some more racist and mysogen...</td>\n",
       "      <td>oh throw racist mysogeny name calls supporter...</td>\n",
       "      <td>oh throw racist mysogeny name call support wo...</td>\n",
       "      <td>oh throw racist mysogeny nam cal support wond...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>-3.306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851263</th>\n",
       "      <td>5162070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>If anyone understands the subtle intricacies o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[If, anyone, understands, the, subtle, intrica...</td>\n",
       "      <td>if anyone understands the subtle intricacies o...</td>\n",
       "      <td>anyone understands subtle intricacies saying o...</td>\n",
       "      <td>anyon understand subtl intricaci say one thing...</td>\n",
       "      <td>anyon understand subtl int say on thing vot an...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-3.441</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196489</th>\n",
       "      <td>5578177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>\"klu klux klan\"?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[\"klu, klux, klan\"?]</td>\n",
       "      <td>klu klux klan</td>\n",
       "      <td>klu klux klan</td>\n",
       "      <td>klu klux klan</td>\n",
       "      <td>klu klux klan</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "488037    842255     0.0  What, you don't think the earth is at least 8,...   \n",
       "851755   5162644     0.0  IMO one reason is the \"degree of foresight\".  ...   \n",
       "830924   5137657     0.0  \" Oh- and throw in some more \"racist\" and \"mys...   \n",
       "851263   5162070     0.0  If anyone understands the subtle intricacies o...   \n",
       "1196489  5578177     0.0                                   \"klu klux klan\"?   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "488037               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "851755               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "830924               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "851263               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "1196489              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "\n",
       "         atheist  ...                                              split  \\\n",
       "488037       NaN  ...  [What,, you, don't, think, the, earth, is, at,...   \n",
       "851755       NaN  ...  [IMO, one, reason, is, the, \"degree, of, fores...   \n",
       "830924       NaN  ...  [\", Oh-, and, throw, in, some, more, \"racist\",...   \n",
       "851263       NaN  ...  [If, anyone, understands, the, subtle, intrica...   \n",
       "1196489      0.0  ...                               [\"klu, klux, klan\"?]   \n",
       "\n",
       "                                       cleaned_w_stopwords  \\\n",
       "488037   what you don't think the earth is at least 8,0...   \n",
       "851755   imo one reason is the degree of foresight  tho...   \n",
       "830924    oh  and throw in some more racist and mysogen...   \n",
       "851263   if anyone understands the subtle intricacies o...   \n",
       "1196489                                      klu klux klan   \n",
       "\n",
       "                                           cleaned_no_stem  \\\n",
       "488037              what think earth least 8,000 years old   \n",
       "851755   imo one reason degree foresight less foresight...   \n",
       "830924    oh throw racist mysogeny name calls supporter...   \n",
       "851263   anyone understands subtle intricacies saying o...   \n",
       "1196489                                      klu klux klan   \n",
       "\n",
       "                                            cleaned_porter  \\\n",
       "488037               what think earth least 8,000 year old   \n",
       "851755   imo one reason degre foresight less foresight ...   \n",
       "830924    oh throw racist mysogeny name call support wo...   \n",
       "851263   anyon understand subtl intricaci say one thing...   \n",
       "1196489                                      klu klux klan   \n",
       "\n",
       "                                         cleaned_lancaster  perc_upper  \\\n",
       "488037                 what think ear least 8,000 year old       0.017   \n",
       "851755   imo on reason degree foresight less foresight ...       0.019   \n",
       "830924    oh throw racist mysogeny nam cal support wond...       0.013   \n",
       "851263   anyon understand subtl int say on thing vot an...       0.015   \n",
       "1196489                                      klu klux klan       0.000   \n",
       "\n",
       "         num_exclam  num_words  perc_stopwords  num_upper_words  \n",
       "488037            0         12          -2.167                0  \n",
       "851755            0         53          -1.660                1  \n",
       "830924            0         36          -3.306                0  \n",
       "851263            0         34          -3.441                0  \n",
       "1196489           0          3          -3.333                0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory issues, we needed to use a smaller training set.  We used a random iid sample of half of the train_sample1 frame to train NB model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_train_sample1 = train_sample1.sample(frac=0.1, replace=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count', 'split',\n",
       "       'cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter',\n",
       "       'cleaned_lancaster', 'perc_upper', 'num_exclam', 'num_words',\n",
       "       'perc_stopwords', 'num_upper_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_train_sample1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are unable to train an NB model on categorical (text) and continuous (numerical) data at the same time, our action plan changed to running two independent models for each type of data and then running a thrid NB model on the resulting predict_proba from the other two trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, train_perc=.80, addtl_feats =[''], model_type = \"Multi\", \n",
    "              num_iter = 10, should_print=False, see_inside=False, comments=\"comment_text\"):\n",
    "    \n",
    "    train_start = 0\n",
    "    train_end = round(model_df.shape[0]*train_perc) \n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = model_df.shape[0]\n",
    "    \n",
    "    X_all = model_df[comments].values\n",
    "    y_all = model_df['target'].values\n",
    "    \n",
    "    # tokenizing text\n",
    "#     count_vect = CountVectorizer()\n",
    "#     X_all_counts = count_vect.fit_transform(X_all.astype('U'))\n",
    "    #print(X_all_counts.shape)\n",
    "\n",
    "    # calculating frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    fitted_vectorizer=tfidf_vectorizer.fit(model_df[comments])\n",
    "    X_all_tfidf =  fitted_vectorizer.transform(model_df[comments])\n",
    "\n",
    "\n",
    "    print(X_all_tfidf.shape)\n",
    "    \n",
    "    if addtl_feats != ['']: # combine non-text and text features if necessary\n",
    "        print(\"here\")\n",
    "#         others_all = model_df[addtl_feats].values.reshape(-1,1)\n",
    "\n",
    "        others_all = model_df[addtl_feats].values.reshape(-1,len(addtl_feats))\n",
    "        #print(others_all)\n",
    "        newfeatures_all = sparse.hstack((X_all_tfidf, others_all.astype(float))).tocsr()\n",
    "    else:\n",
    "        newfeatures_all = X_all_tfidf\n",
    "    \n",
    "    \n",
    "    X_train = newfeatures_all[train_start:train_end]\n",
    "    y_train = model_df[train_start:train_end]['target'].values\n",
    "    y_train=y_train.astype('int')\n",
    "\n",
    "    X_test = newfeatures_all[test_start:test_end]\n",
    "    y_test = model_df[test_start:test_end]['target'].values\n",
    "    \n",
    "    \n",
    "    if model_type == 'Multi':\n",
    "        clf = MultinomialNB().fit(X_train, y_train)\n",
    "    if model_type == \"Gauss\":\n",
    "        clf = GaussianNB().fit(X_train, y_train) \n",
    "   \n",
    "    predicted = clf.predict(X_test)\n",
    "    accuracy = np.mean(predicted == y_test)\n",
    "    \n",
    "\n",
    "#     y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "#     precision = precision_score(y_true_sorted, preds)\n",
    "\n",
    "\n",
    "    if should_print == True:\n",
    "\n",
    "        print(\"The accuracy on the test set is {}%.\".format(round(accuracy*100,2)))    \n",
    "    \n",
    "    if see_inside == True:\n",
    "        return clf, accuracy, X_all_counts, X_all_tfidf\n",
    "    else:\n",
    "        return clf, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72244, 65038)\n",
      "The accuracy on the test set is 70.18%.\n"
     ]
    }
   ],
   "source": [
    "clf1, accuracy = run_model(subset_train_sample1, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72244, 65038)\n",
      "here\n",
      "The accuracy on the test set is 70.18%.\n"
     ]
    }
   ],
   "source": [
    "clf2, accuracy = run_model(subset_train_sample1, should_print=True, addtl_feats =['insult', 'perc_upper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_w_stopwords\n",
      "(72244, 64969)\n",
      "The accuracy on the test set is 70.18%.\n",
      "cleaned_no_stem\n",
      "(72244, 64967)\n",
      "The accuracy on the test set is 70.18%.\n",
      "cleaned_porter\n",
      "(72244, 61279)\n",
      "The accuracy on the test set is 70.18%.\n",
      "cleaned_lancaster\n",
      "(72244, 58684)\n",
      "The accuracy on the test set is 70.18%.\n"
     ]
    }
   ],
   "source": [
    "for text in ['cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter','cleaned_lancaster']:\n",
    "    print(text)\n",
    "    run_model(subset_train_sample1, comments=text, should_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    `"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
