{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, a Naive Bayes model is run on a iid sampled data set of approximately 250K rows of data.  This notebook was run on an AWS SageMaker ml.c5.4xlarge instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import feature_generation_functions as fgf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_functions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle_functions as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label comments as toxic (\"1\") or nontoxic (\"0\") using 0.5 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_set and validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Citation: https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas\n",
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "hold_out_set = train[~msk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1358996\n",
      "1      85262\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    339440\n",
      "1     21176\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(hold_out_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sample train_set to create a smaller data frame (train_sample) to run NB on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_set.sample(frac=0.75, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1018765\n",
      "1      64429\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_sample.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned with stopwords...Elapsed Time:  0.265 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.368 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  8.513 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  7.019 minutes\n",
      "\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "train_df = fgf.generate_NB_SVM_features(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled and sent to bucket!\n"
     ]
    }
   ],
   "source": [
    "pf.write_pickle_to_s3bucket(filename='NB_final_1M', \n",
    "                            bucket_name='advancedml-koch-mathur-hinkson', \n",
    "                            df=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned with stopwords...Elapsed Time:  0.088 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.123 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  2.788 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  2.291 minutes\n",
      "\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "hold_out_df = fgf.generate_NB_SVM_features(hold_out_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickled and sent to bucket!\n"
     ]
    }
   ],
   "source": [
    "pf.write_pickle_to_s3bucket(filename='NB_final_holdout_350K', \n",
    "                            bucket_name='advancedml-koch-mathur-hinkson', \n",
    "                            df=hold_out_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping & Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pf.read_pickle(filename='NB_final_1M', bucket_name='advancedml-koch-mathur-hinkson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_df = pf.read_pickle(filename='NB_final_holdout_350K', bucket_name='advancedml-koch-mathur-hinkson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_df[train_df.toxicity_category == 1]\n",
    "nontoxic = train_df[train_df.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1083194, 50), (64429, 50), (1018765, 50))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the dataset to include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = len(toxic)\n",
    "ten_percent = round((len(toxic) / 5) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25772"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_percent * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = train_df.sample(quarter*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    128858\n",
      "0    128858\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_50 = toxic.append(toxic).append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_50 = prepared_50.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_50.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    193524\n",
      "1     96406\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_40 = toxic.append(toxic).append(toxic).append(nontoxic.sample(len(toxic) * 6))\n",
    "prepared_40 = prepared_40.sample(frac=.5).reset_index(drop=True)\n",
    "print(prepared_40.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3325147449384334"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96406 / (193524 + 96406)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From NB_iter4 notebook we learned that cleaned_no_stem_str was the feature that yeilded the highest performing model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model now\n"
     ]
    }
   ],
   "source": [
    "classifier, output, fitted_vectorizer = mf.run_model(model_df=prepared_50, \n",
    "                                                     model_type=\"MultiNB\", \n",
    "                                                     comments = \"cleaned_no_stem_str\", \n",
    "                                                     train_perc=0.8, \n",
    "                                                     target=\"toxicity_category\", \n",
    "                                                     see_inside=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.8231573637545351\n",
      "Overall Precision: 0.7878411910669976\n",
      "Overall Recall: 0.88539447693559\n",
      "Overall F1 Score: 0.8337740494209903\n",
      "ROC_AUC: 0.823\n",
      "\n",
      "Target Accuracy: 0.88539447693559\n",
      "Target Precision: 1.0\n",
      "Target Recall: 0.88539447693559\n",
      "Target F1 Score: 0.9392140347994002\n",
      "\n",
      "Non-Target Accuracy: 0.7606904058466801\n",
      "Non-Target Precision: 1.0\n",
      "Non-Target Recall: 0.7606904058466801\n",
      "Non-Target F1 Score: 0.8640819570785128\n",
      "\n",
      "Strong Identity Accuracy: 0.9066666666666666\n",
      "Strong Identity Precision: 0.9980818414322251\n",
      "Strong Identity Recall: 0.908086096567772\n",
      "Strong Identity F1 Score: 1.0\n",
      "\n",
      "Obscenity Accuracy: 0.8784\n",
      "Obscenity Precision: 0.9993932038834952\n",
      "Obscenity Recall: 0.8788687299893276\n",
      "Obscenity F1 Score: 1.0\n",
      "\n",
      "Insults Accuracy: 0.8975253807106599\n",
      "Insults Precision: 0.9990581032554307\n",
      "Insults Recall: 0.8982692002328905\n",
      "Insults F1 Score: 1.0\n",
      "\n",
      "Threats Accuracy: 0.8932178932178932\n",
      "Threats Precision: 0.9983870967741936\n",
      "Threats Recall: 0.8945086705202312\n",
      "Threats F1 Score: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Overall': {'Accuracy': 0.8231573637545351,\n",
       "  'Precision': 0.7878411910669976,\n",
       "  'Recall': 0.88539447693559,\n",
       "  'F1': 0.8337740494209903,\n",
       "  'ROC_AUC': 0.823},\n",
       " 'Target': {'Accuracy': 0.88539447693559,\n",
       "  'Precision': 1.0,\n",
       "  'Recall': 0.88539447693559,\n",
       "  'F1': 0.9392140347994002},\n",
       " 'Non-Target': {'Accuracy': 0.8932178932178932,\n",
       "  'Precision': 0.9983870967741936,\n",
       "  'Recall': 0.8945086705202312,\n",
       "  'F1': 1.0}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mf.get_metrics(output=output, detailed=True, should_print=True, round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
      "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
      "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
      "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
      "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
      "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
      "       'other_sexual_orientation', 'physical_disability',\n",
      "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
      "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
      "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
      "       'identity_annotator_count', 'toxicity_annotator_count',\n",
      "       'toxicity_category', 'cleaned_w_stopwords_str', 'cleaned_no_stem_str',\n",
      "       'cleaned_porter_str', 'cleaned_lancaster_str', 'predicted', 'y_test'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "hold_out_results = mf.run_model_test(model_df=hold_out_df, \n",
    "                                     clf=classifier, \n",
    "                                     vectorizer=fitted_vectorizer, \n",
    "                                     comments=\"cleaned_no_stem_str\", target=\"toxicity_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_out_results.to_csv(\"holdout_results\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    339440\n",
       "1     21176\n",
       "Name: toxicity_category, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold_out_results.toxicity_category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100085, 52)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold_out_results[hold_out_results.predicted == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17732, 52)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold_out_results[(hold_out_results.predicted == 1) & (hold_out_results.toxicity_category == 1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17716940600489584"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(hold_out_results.y_test, hold_out_results.predicted, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.762082103955454\n",
      "Overall Precision: 0.17716940600489584\n",
      "Overall Recall: 0.837363052512278\n",
      "Overall F1 Score: 0.29246006547859577\n",
      "ROC_AUC: 0.797\n",
      "\n",
      "Target Accuracy: 0.837363052512278\n",
      "Target Precision: 1.0\n",
      "Target Recall: 0.837363052512278\n",
      "Target F1 Score: 0.9114834995373703\n",
      "\n",
      "Non-Target Accuracy: 0.7573856940843743\n",
      "Non-Target Precision: 1.0\n",
      "Non-Target Recall: 0.7573856940843743\n",
      "Non-Target F1 Score: 0.8619458968328341\n",
      "\n",
      "Strong Identity Accuracy: 0.8355525965379494\n",
      "Strong Identity Precision: 0.9436728395061729\n",
      "Strong Identity Recall: 0.8754473872584109\n",
      "Strong Identity F1 Score: 1.0\n",
      "\n",
      "Obscenity Accuracy: 0.8239482200647249\n",
      "Obscenity Precision: 0.9936958234830575\n",
      "Obscenity Recall: 0.8268852459016394\n",
      "Obscenity F1 Score: 1.0\n",
      "\n",
      "Insults Accuracy: 0.842698752677334\n",
      "Insults Precision: 0.9814458900059136\n",
      "Insults Recall: 0.8553114732976873\n",
      "Insults F1 Score: 1.0\n",
      "\n",
      "Threats Accuracy: 0.8333333333333334\n",
      "Threats Precision: 0.9782135076252724\n",
      "Threats Recall: 0.8471698113207548\n",
      "Threats F1 Score: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hold_out_metrics = mf.get_metrics(output=hold_out_results, detailed=True, should_print=True, round_to=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
