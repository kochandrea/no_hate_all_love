{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:20.4f}'.format\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import time\n",
    "\n",
    "# import io\n",
    "# import boto3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle_functions as pf #has io, boto3, boto3.session, _pickle, pandas\n",
    "import feature_generation_functions as fgf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97320, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (4874, 45)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000  #chunk row size\n",
    "train_sub_dfs = [train[i:i+n] for i in range(0,train.shape[0],n)]\n",
    "\n",
    "[i.shape for i in train_sub_dfs]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# intialize stemmer\n",
    "ps = PorterStemmer() \n",
    "ls = LancasterStemmer()\n",
    "\n",
    "# define stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('')\n",
    "\n",
    "approved_stop_words = {\"not\", \"get\", \"against\", \"haven\", \"haven't\",\"aren't\", \n",
    "                       \"aren\", \"should\", \"shouldn\", \"shouldn't\", \"themselves\", \n",
    "                       \"them\", \"under\", \"over\", 'won', \"won't\", \"wouldn'\", \n",
    "                       \"wouldn't\"}\n",
    "\n",
    "stops = stops - approved_stop_words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def clean_text(text, stop_ws=stops, stemmer=ps, str_output=True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "    \n",
    "    if stop_ws:\n",
    "        t = [w.lower() for w in t if w not in stop_ws]\n",
    "    \n",
    "    if stemmer:\n",
    "        t = [stemmer.stem(w) for w in t]\n",
    "    \n",
    "    if str_output:\n",
    "        return ' '.join(t)\n",
    "    else:\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def make_ngrams(preprocessed, n=2, str_output=True):\n",
    "    '''\n",
    "    Covert a list of preprocessed strings into ngrams of length n.\n",
    "    Should return X ngrams of X words less (n - 1).\n",
    "    '''\n",
    "    ngrams_tuples = []\n",
    "\n",
    "    # ensure that all ngrams are of length n by specifying list position of\n",
    "    # first item in last ngram\n",
    "    last_ngram_start = len(preprocessed) - (n - 1)\n",
    "\n",
    "    # for each string from position i through last ngram start position, create\n",
    "    # a tuple of length n\n",
    "    for i in range(last_ngram_start):\n",
    "        ngrams_tuples.append(tuple(preprocessed[i:i + n]))\n",
    "    if str_output:\n",
    "        return [' '.join(ngram) for ngram in ngrams_tuples]\n",
    "    else:\n",
    "        return ngrams_tuples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def print_elapsed_time(start, end, m):\n",
    "    print(f\"{m}...Elapsed Time:  {round((end - start)/60,3)} minutes\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_features(df):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    split_time = time.perf_counter()\n",
    "    print_elapsed_time(start_time, split_time, m=\"Split comments\")\n",
    "\n",
    "    df['cleaned_w_stopwords_str'] = df[\"comment_text\"].apply(clean_text,args=(None,None,True),)\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,None,False),)\n",
    "    with_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(split_time, with_stopwords, m=\"Cleaned with stopwords\")\n",
    "\n",
    "    df['cleaned_no_stem_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,None, True),)\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,args=(stops,None,False),)\n",
    "    without_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(with_stopwords, without_stopwords, m=\"Cleaned without stopwords\")\n",
    "\n",
    "    df['cleaned_porter_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,True),)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,False),)\n",
    "    porter_time = time.perf_counter()\n",
    "    print_elapsed_time(without_stopwords, porter_time, m=\"Stemmed (Porter)\")\n",
    "\n",
    "    df['cleaned_lancaster_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,True),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,False),)\n",
    "    lancaster_time = time.perf_counter()\n",
    "    print_elapsed_time(porter_time, lancaster_time, m=\"Stemmed (Lancaster)\")\n",
    "\n",
    "    df['bigrams_unstemmed'] = df[\"cleaned_no_stem\"].apply(make_ngrams,args=(2, True),)\n",
    "    bigrams_time = time.perf_counter()\n",
    "    print_elapsed_time(lancaster_time, bigrams_time, m=\"Created bigrams\")\n",
    "    # df['trigram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(5, True),)\n",
    "    #\n",
    "    # df['bigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(2, True),)\n",
    "    # df['trigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(5, True),)\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: 0 if x == 0 else round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "    pct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(bigrams_time, pct_upper_time, m=\"Calculated uppercase pct\")\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    punctuation_time = time.perf_counter()\n",
    "    print_elapsed_time(pct_upper_time, punctuation_time, m=\"Count punctuation\")\n",
    "\n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    wordcount_time = time.perf_counter()\n",
    "    print_elapsed_time(punctuation_time, wordcount_time, m=\"Count words\")\n",
    "\n",
    "    calc_stopwords_pct = lambda x, y: 0 if y == 0 else round((x - len(y)) / x, 3)\n",
    "    df['perc_stopwords'] = df[[\"num_words\", \"cleaned_no_stem\"]].apply(lambda x: calc_stopwords_pct(*x), axis=1)\n",
    "    stops_pct_time = time.perf_counter()\n",
    "    print_elapsed_time(wordcount_time, stops_pct_time, m=\"Count stopwords pct\")\n",
    "\n",
    "    df['num_upper_words'] = df[\"split\"].apply(lambda x: sum(map(str.isupper, x)) )\n",
    "    ct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(stops_pct_time, ct_upper_time, m=\"Count uppercase words\")\n",
    "    \n",
    "    print()\n",
    "    print(\"DONE GENERATING FEATURES\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def pickle_to_s3bucket(filename_string, df, bucket_string):\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = bucket_string\n",
    "    key = filename_string\n",
    "\n",
    "    # test.to_pickle(pickle_buffer)\n",
    "    # s3_resource.Object(bucket, 'full_preprocessed_test.pkl').put(Body=pickle_buffer.getvalue())\n",
    "\n",
    "    df.to_pickle(key)\n",
    "    s3_resource.Object(bucket,key).put(Body=open(key, 'rb'))\n",
    "    print(\"sent to bucket\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:0.01 minutes\n",
      "Cleaned with stopwords...Elapsed Time:0.059 minutes\n",
      "Cleaned without stopwords...Elapsed Time:0.082 minutes\n",
      "Stemmed (Porter)...Elapsed Time:2.248 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:1.815 minutes\n",
      "Created bigrams...Elapsed Time:4.25 minutes\n",
      "Calculated uppercase pct...Elapsed Time:0.009 minutes\n",
      "Count punctuation...Elapsed Time:0.003 minutes\n",
      "Count words...Elapsed Time:0.001 minutes\n",
      "Count stopwords pct...Elapsed Time:0.035 minutes\n",
      "Count uppercase words...Elapsed Time:0.008 minutes\n"
     ]
    }
   ],
   "source": [
    "test_preprocessed = add_text_cleaning_cols(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed.to_pickle('test_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.065 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.083 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.618 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.326 minutes\n",
      "Created bigrams...Elapsed Time:  0.035 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.019 minutes\n",
      "Count punctuation...Elapsed Time:  0.013 minutes\n",
      "Count words...Elapsed Time:  0.012 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n"
     ]
    }
   ],
   "source": [
    "sub_train_df1 = train_sub_dfs[0]\n",
    "sub_train_df1_preprocessed = generate_features(sub_train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df1_preprocessed.to_pickle('sub_train_df1_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.067 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.084 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.652 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.354 minutes\n",
      "Created bigrams...Elapsed Time:  0.034 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "sub_train_df2 = train_sub_dfs[1]\n",
    "sub_train_df2_preprocessed = generate_features(sub_train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df2_preprocessed.to_pickle('sub_train_df2_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.013 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.066 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.085 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.645 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.36 minutes\n",
      "Created bigrams...Elapsed Time:  0.034 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "sub_train_df3 = train_sub_dfs[2]\n",
    "sub_train_df3_preprocessed = generate_features(sub_train_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df3_preprocessed.to_pickle('sub_train_df3_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.064 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.081 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.572 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.329 minutes\n",
      "Created bigrams...Elapsed Time:  0.033 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.017 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "sub_train_df4 = train_sub_dfs[3]\n",
    "sub_train_df4_preprocessed = generate_features(sub_train_df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df4_preprocessed.to_pickle('sub_train_df4_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.063 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.08 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.533 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.299 minutes\n",
      "Created bigrams...Elapsed Time:  0.033 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.039 minutes\n",
      "Count uppercase words...Elapsed Time:  0.017 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "sub_train_df5 = train_sub_dfs[4]\n",
    "sub_train_df5_preprocessed = generate_features(sub_train_df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df5_preprocessed.to_pickle('sub_train_df5_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.013 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.062 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.078 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.514 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.277 minutes\n",
      "Created bigrams...Elapsed Time:  0.033 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.017 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    }
   ],
   "source": [
    "sub_train_df6 = train_sub_dfs[5]\n",
    "sub_train_df6_preprocessed = generate_features(sub_train_df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_s3bucket('sub_train_df6_preprocessed.pkl', sub_train_df6_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.064 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.081 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.567 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.319 minutes\n",
      "Created bigrams...Elapsed Time:  0.034 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.019 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.012 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n",
      "DONE GENERATING FEATURES\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle_to_s3bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4b7b1418cf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub_train_df7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sub_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msub_train_df7_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_df7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle_to_s3bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sub_train_df7_preprocessed.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_train_df7_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sent to bucket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle_to_s3bucket' is not defined"
     ]
    }
   ],
   "source": [
    "sub_train_df7 = train_sub_dfs[6]\n",
    "sub_train_df7_preprocessed = generate_features(sub_train_df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_s3bucket('sub_train_df7_preprocessed.pkl', sub_train_df7_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.014 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.065 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.083 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.574 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.348 minutes\n",
      "Created bigrams...Elapsed Time:  0.034 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.017 minutes\n",
      "DONE GENERATING FEATURES\n",
      "sent to bucket\n"
     ]
    }
   ],
   "source": [
    "sub_train_df8 = train_sub_dfs[7]\n",
    "sub_train_df8_preprocessed = generate_features(sub_train_df8)\n",
    "pickle_to_s3bucket('sub_train_df8_preprocessed.pkl', sub_train_df8_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_to_s3bucket('sub_train_df8_preprocessed.pkl', sub_train_df8_preprocessed, 'advancedml-koch-mathur-hinkson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.015 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.067 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.086 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.611 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.361 minutes\n",
      "Created bigrams...Elapsed Time:  0.035 minutes\n",
      "Calculated uppercase pct...Elapsed Time:  0.019 minutes\n",
      "Count punctuation...Elapsed Time:  0.013 minutes\n",
      "Count words...Elapsed Time:  0.012 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n",
      "\n",
      "DONE GENERATING FEATURES\n",
      "Pickled and sent to bucket!\n"
     ]
    }
   ],
   "source": [
    "sub_train_df9 = train_sub_dfs[8]\n",
    "sub_train_df9_preprocessed = fgf.generate_features(sub_train_df9)\n",
    "pf.pickle_to_s3bucket('sub_train_df9_preprocessed.pkl', sub_train_df9_preprocessed, 'advancedml-koch-mathur-hinkson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
