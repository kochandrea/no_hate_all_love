{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:20.4f}'.format\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97320, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (4874, 45)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000  #chunk row size\n",
    "train_sub_dfs = [train[i:i+n] for i in range(0,train.shape[0],n)]\n",
    "\n",
    "[i.shape for i in train_sub_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize stemmer\n",
    "ps = PorterStemmer() \n",
    "ls = LancasterStemmer()\n",
    "\n",
    "# define stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('')\n",
    "\n",
    "approved_stop_words = {\"not\", \"get\", \"against\", \"haven\", \"haven't\",\"aren't\", \n",
    "                       \"aren\", \"should\", \"shouldn\", \"shouldn't\", \"themselves\", \n",
    "                       \"them\", \"under\", \"over\", 'won', \"won't\", \"wouldn'\", \n",
    "                       \"wouldn't\"}\n",
    "\n",
    "stops = stops - approved_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_ws=stops, stemmer=ps, str_output=True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "    \n",
    "    if stop_ws:\n",
    "        t = [w.lower() for w in t if w not in stop_ws]\n",
    "    \n",
    "    if stemmer:\n",
    "        t = [stemmer.stem(w) for w in t]\n",
    "    \n",
    "    if str_output:\n",
    "        return ' '.join(t)\n",
    "    else:\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(preprocessed, n=2, str_output=True):\n",
    "    '''\n",
    "    Covert a list of preprocessed strings into ngrams of length n.\n",
    "    Should return X ngrams of X words less (n - 1).\n",
    "    '''\n",
    "    ngrams_tuples = []\n",
    "\n",
    "    # ensure that all ngrams are of length n by specifying list position of\n",
    "    # first item in last ngram\n",
    "    last_ngram_start = len(preprocessed) - (n - 1)\n",
    "\n",
    "    # for each string from position i through last ngram start position, create\n",
    "    # a tuple of length n\n",
    "    for i in range(last_ngram_start):\n",
    "        ngrams_tuples.append(tuple(preprocessed[i:i + n]))\n",
    "    if str_output:\n",
    "        return [' '.join(ngram) for ngram in ngrams_tuples]\n",
    "    else:\n",
    "        return ngrams_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(start, end, m):\n",
    "    print(f\"{m}...Elapsed Time:  {round((end - start)/60,3)} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_features(df):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    split_time = time.perf_counter()\n",
    "    print_elapsed_time(start_time, split_time, m=\"Split comments\")\n",
    "\n",
    "    df['cleaned_w_stopwords_str'] = df[\"comment_text\"].apply(clean_text,args=(None,None,True),)\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,None,False),)\n",
    "    with_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(split_time, with_stopwords, m=\"Cleaned with stopwords\")\n",
    "\n",
    "    df['cleaned_no_stem_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,None, True),)\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,args=(stops,None,False),)\n",
    "    without_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(with_stopwords, without_stopwords, m=\"Cleaned without stopwords\")\n",
    "\n",
    "    df['cleaned_porter_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,True),)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,False),)\n",
    "    porter_time = time.perf_counter()\n",
    "    print_elapsed_time(without_stopwords, porter_time, m=\"Stemmed (Porter)\")\n",
    "\n",
    "    df['cleaned_lancaster_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,True),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,False),)\n",
    "    lancaster_time = time.perf_counter()\n",
    "    print_elapsed_time(porter_time, lancaster_time, m=\"Stemmed (Lancaster)\")\n",
    "\n",
    "    df['bigrams_unstemmed'] = df[\"cleaned_no_stem\"].apply(make_ngrams,args=(2, True),)\n",
    "    bigrams_time = time.perf_counter()\n",
    "    print_elapsed_time(lancaster_time, bigrams_time, m=\"Created bigrams\")\n",
    "    # df['trigram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(5, True),)\n",
    "    #\n",
    "    # df['bigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(2, True),)\n",
    "    # df['trigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(5, True),)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuous_features(df):\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: 0 if x == 0 else round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "    pct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(bigrams_time, pct_upper_time, m=\"Calculated uppercase pct\")\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    punctuation_time = time.perf_counter()\n",
    "    print_elapsed_time(pct_upper_time, punctuation_time, m=\"Count punctuation\")\n",
    "\n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    wordcount_time = time.perf_counter()\n",
    "    print_elapsed_time(punctuation_time, wordcount_time, m=\"Count words\")\n",
    "\n",
    "    calc_stopwords_pct = lambda x, y: 0 if y == 0 else round((x - len(y)) / x, 3)\n",
    "    df['perc_stopwords'] = df[[\"num_words\", \"cleaned_no_stem\"]].apply(lambda x: calc_stopwords_pct(*x), axis=1)\n",
    "    stops_pct_time = time.perf_counter()\n",
    "    print_elapsed_time(wordcount_time, stops_pct_time, m=\"Count stopwords pct\")\n",
    "\n",
    "    df['num_upper_words'] = df[\"split\"].apply(lambda x: sum(map(str.isupper, x)) )\n",
    "    ct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(stops_pct_time, ct_upper_time, m=\"Count uppercase words\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:0.01 minutes\n",
      "Cleaned with stopwords...Elapsed Time:0.059 minutes\n",
      "Cleaned without stopwords...Elapsed Time:0.082 minutes\n",
      "Stemmed (Porter)...Elapsed Time:2.248 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:1.815 minutes\n",
      "Created bigrams...Elapsed Time:4.25 minutes\n",
      "Calculated uppercase pct...Elapsed Time:0.009 minutes\n",
      "Count punctuation...Elapsed Time:0.003 minutes\n",
      "Count words...Elapsed Time:0.001 minutes\n",
      "Count stopwords pct...Elapsed Time:0.035 minutes\n",
      "Count uppercase words...Elapsed Time:0.008 minutes\n"
     ]
    }
   ],
   "source": [
    "test_preprocessed = add_text_cleaning_cols(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed.to_pickle('test_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = train.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:1.369 minutes\n",
      "Cleaned with stopwords...Elapsed Time:2.512 minutes\n",
      "Cleaned without stopwords...Elapsed Time:2.426 minutes\n",
      "Stemmed (Porter)...Elapsed Time:2.696 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:2.899 minutes\n",
      "Created bigrams...Elapsed Time:13.037 minutes\n",
      "Calculated uppercase pct...Elapsed Time:1.193 minutes\n",
      "Count punctuation...Elapsed Time:1.187 minutes\n",
      "Count words...Elapsed Time:1.183 minutes\n",
      "Count stopwords pct...Elapsed Time:1.165 minutes\n",
      "Count uppercase words...Elapsed Time:1.236 minutes\n"
     ]
    }
   ],
   "source": [
    "mini_preprocessed = add_text_cleaning_cols(mini) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_preprocessed.to_pickle('mini_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = gener "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed.to_pickle('train_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
