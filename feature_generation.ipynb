{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.float_format = '{:20.4f}'.format\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97320, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874, 45)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (100000, 45),\n",
       " (4874, 45)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 100000  #chunk row size\n",
    "train_sub_dfs = [train[i:i+n] for i in range(0,train.shape[0],n)]\n",
    "\n",
    "[i.shape for i in train_sub_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize stemmer\n",
    "ps = PorterStemmer() \n",
    "ls = LancasterStemmer()\n",
    "\n",
    "# define stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "stops.add('')\n",
    "\n",
    "approved_stop_words = {\"not\", \"get\", \"against\", \"haven\", \"haven't\",\"aren't\", \n",
    "                       \"aren\", \"should\", \"shouldn\", \"shouldn't\", \"themselves\", \n",
    "                       \"them\", \"under\", \"over\", 'won', \"won't\", \"wouldn'\", \n",
    "                       \"wouldn't\"}\n",
    "\n",
    "stops = stops - approved_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_ws=stops, stemmer=ps, str_output=True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "    \n",
    "    if stop_ws:\n",
    "        t = [w.lower() for w in t if w not in stop_ws]\n",
    "    \n",
    "    if stemmer:\n",
    "        t = [stemmer.stem(w) for w in t]\n",
    "    \n",
    "    if str_output:\n",
    "        return ' '.join(t)\n",
    "    else:\n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(preprocessed, n=2, str_output=True):\n",
    "    '''\n",
    "    Covert a list of preprocessed strings into ngrams of length n.\n",
    "    Should return X ngrams of X words less (n - 1).\n",
    "    '''\n",
    "    ngrams_tuples = []\n",
    "\n",
    "    # ensure that all ngrams are of length n by specifying list position of\n",
    "    # first item in last ngram\n",
    "    last_ngram_start = len(preprocessed) - (n - 1)\n",
    "\n",
    "    # for each string from position i through last ngram start position, create\n",
    "    # a tuple of length n\n",
    "    for i in range(last_ngram_start):\n",
    "        ngrams_tuples.append(tuple(preprocessed[i:i + n]))\n",
    "    if str_output:\n",
    "        return [' '.join(ngram) for ngram in ngrams_tuples]\n",
    "    else:\n",
    "        return ngrams_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_elapsed_time(start, end, m):\n",
    "    print(f\"{m}...Elapsed Time:  {round((end - start)/60,3)} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    split_time = time.perf_counter()\n",
    "    print_elapsed_time(start_time, split_time, m=\"Split comments\")\n",
    "\n",
    "    df['cleaned_w_stopwords_str'] = df[\"comment_text\"].apply(clean_text,args=(None,None,True),)\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,None,False),)\n",
    "    with_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(split_time, with_stopwords, m=\"Cleaned with stopwords\")\n",
    "\n",
    "    df['cleaned_no_stem_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,None, True),)\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,args=(stops,None,False),)\n",
    "    without_stopwords = time.perf_counter()\n",
    "    print_elapsed_time(with_stopwords, without_stopwords, m=\"Cleaned without stopwords\")\n",
    "\n",
    "    df['cleaned_porter_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,True),)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(stops,ps,False),)\n",
    "    porter_time = time.perf_counter()\n",
    "    print_elapsed_time(without_stopwords, porter_time, m=\"Stemmed (Porter)\")\n",
    "\n",
    "    df['cleaned_lancaster_str'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,True),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(stops,ls,False),)\n",
    "    lancaster_time = time.perf_counter()\n",
    "    print_elapsed_time(porter_time, lancaster_time, m=\"Stemmed (Lancaster)\")\n",
    "\n",
    "    df['bigrams_unstemmed'] = df[\"cleaned_no_stem\"].apply(make_ngrams,args=(2, True),)\n",
    "    bigrams_time = time.perf_counter()\n",
    "    print_elapsed_time(lancaster_time, bigrams_time, m=\"Created bigrams\")\n",
    "    # df['trigram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_porter'] = df[\"cleaned_porter\"].apply(make_ngrams,args=(5, True),)\n",
    "    #\n",
    "    # df['bigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(2, True),)\n",
    "    # df['trigram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(3, True),)\n",
    "    # df['fourgram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(4, True),)\n",
    "    # df['fivegram_lancaster'] = df[\"cleaned_lancaster\"].apply(make_ngrams,args=(5, True),)\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: 0 if x == 0 else round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "    pct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(bigrams_time, pct_upper_time, m=\"Calculated uppercase pct\")\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    punctuation_time = time.perf_counter()\n",
    "    print_elapsed_time(pct_upper_time, punctuation_time, m=\"Count punctuation\")\n",
    "\n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    wordcount_time = time.perf_counter()\n",
    "    print_elapsed_time(punctuation_time, wordcount_time, m=\"Count words\")\n",
    "\n",
    "    calc_stopwords_pct = lambda x, y: 0 if y == 0 else round((x - len(y)) / x, 3)\n",
    "    df['perc_stopwords'] = df[[\"num_words\", \"cleaned_no_stem\"]].apply(lambda x: calc_stopwords_pct(*x), axis=1)\n",
    "    stops_pct_time = time.perf_counter()\n",
    "    print_elapsed_time(wordcount_time, stops_pct_time, m=\"Count stopwords pct\")\n",
    "\n",
    "    df['num_upper_words'] = df[\"split\"].apply(lambda x: sum(map(str.isupper, x)) )\n",
    "    ct_upper_time = time.perf_counter()\n",
    "    print_elapsed_time(stops_pct_time, ct_upper_time, m=\"Count uppercase words\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:0.01 minutes\n",
      "Cleaned with stopwords...Elapsed Time:0.059 minutes\n",
      "Cleaned without stopwords...Elapsed Time:0.082 minutes\n",
      "Stemmed (Porter)...Elapsed Time:2.248 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:1.815 minutes\n",
      "Created bigrams...Elapsed Time:4.25 minutes\n",
      "Calculated uppercase pct...Elapsed Time:0.009 minutes\n",
      "Count punctuation...Elapsed Time:0.003 minutes\n",
      "Count words...Elapsed Time:0.001 minutes\n",
      "Count stopwords pct...Elapsed Time:0.035 minutes\n",
      "Count uppercase words...Elapsed Time:0.008 minutes\n"
     ]
    }
   ],
   "source": [
    "test_preprocessed = add_text_cleaning_cols(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocessed.to_pickle('test_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = train.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:1.369 minutes\n",
      "Cleaned with stopwords...Elapsed Time:2.512 minutes\n",
      "Cleaned without stopwords...Elapsed Time:2.426 minutes\n",
      "Stemmed (Porter)...Elapsed Time:2.696 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:2.899 minutes\n",
      "Created bigrams...Elapsed Time:13.037 minutes\n",
      "Calculated uppercase pct...Elapsed Time:1.193 minutes\n",
      "Count punctuation...Elapsed Time:1.187 minutes\n",
      "Count words...Elapsed Time:1.183 minutes\n",
      "Count stopwords pct...Elapsed Time:1.165 minutes\n",
      "Count uppercase words...Elapsed Time:1.236 minutes\n"
     ]
    }
   ],
   "source": [
    "mini_preprocessed = add_text_cleaning_cols(mini) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_preprocessed.to_pickle('mini_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_df1 = train_sub_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.8723</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               target  \\\n",
       "0  59848               0.0000   \n",
       "1  59849               0.0000   \n",
       "2  59852               0.0000   \n",
       "3  59855               0.0000   \n",
       "4  59856               0.8936   \n",
       "\n",
       "                                        comment_text      severe_toxicity  \\\n",
       "0  This is so cool. It's like, 'would you want yo...               0.0000   \n",
       "1  Thank you!! This would make my life a lot less...               0.0000   \n",
       "2  This is such an urgent design problem; kudos t...               0.0000   \n",
       "3  Is this something I'll be able to install on m...               0.0000   \n",
       "4               haha you guys are a bunch of losers.               0.0213   \n",
       "\n",
       "               obscene      identity_attack               insult  \\\n",
       "0               0.0000               0.0000               0.0000   \n",
       "1               0.0000               0.0000               0.0000   \n",
       "2               0.0000               0.0000               0.0000   \n",
       "3               0.0000               0.0000               0.0000   \n",
       "4               0.0000               0.0213               0.8723   \n",
       "\n",
       "                threat                asian              atheist  ...  \\\n",
       "0               0.0000                  nan                  nan  ...   \n",
       "1               0.0000                  nan                  nan  ...   \n",
       "2               0.0000                  nan                  nan  ...   \n",
       "3               0.0000                  nan                  nan  ...   \n",
       "4               0.0000               0.0000               0.0000  ...   \n",
       "\n",
       "   article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0        2006  rejected      0    0    0      0         0   \n",
       "1        2006  rejected      0    0    0      0         0   \n",
       "2        2006  rejected      0    0    0      0         0   \n",
       "3        2006  rejected      0    0    0      0         0   \n",
       "4        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "       sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0               0.0000                         0                         4  \n",
       "1               0.0000                         0                         4  \n",
       "2               0.0000                         0                         4  \n",
       "3               0.0000                         0                         4  \n",
       "4               0.0000                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split comments...Elapsed Time:  0.013 minutes\n",
      "Cleaned with stopwords...Elapsed Time:  0.065 minutes\n",
      "Cleaned without stopwords...Elapsed Time:  0.084 minutes\n",
      "Stemmed (Porter)...Elapsed Time:  1.609 minutes\n",
      "Stemmed (Lancaster)...Elapsed Time:  1.364 minutes\n",
      "Created bigrams...Elapsed Time:  0.034 minutes\n"
     ]
    }
   ],
   "source": [
    "sub_train_df1_preprocessed = generate_categorical_features(sub_train_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 60)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_train_df1_preprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated uppercase pct...Elapsed Time:  0.018 minutes\n",
      "Count punctuation...Elapsed Time:  0.012 minutes\n",
      "Count words...Elapsed Time:  0.011 minutes\n",
      "Count stopwords pct...Elapsed Time:  0.038 minutes\n",
      "Count uppercase words...Elapsed Time:  0.018 minutes\n"
     ]
    }
   ],
   "source": [
    "sub_train_df1_preprocessed = generate_continuous_features(sub_train_df1_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed.to_pickle('train_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
