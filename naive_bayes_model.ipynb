{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called \"toxicity_category\" in the train data frame categorizing comments as toxic (\"1\") or non-toxic (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into training (80%) and validation sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "validation_set = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1358961\n",
      "1      84890\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    339475\n",
      "1     21548\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validation_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small sample (\"train_sample1\") from the train_set on which to run models.  Ensure that samples are iid by replacing after each draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_set.sample(frac=0.1, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    135745\n",
      "1      8640\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_sample.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.add('')\n",
    "\n",
    "def clean_text(text, stemming=None, remove_sw = True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    \n",
    "    t = [w.lower() for w in t]\n",
    "    \n",
    "    if remove_sw == True:\n",
    "        t = [w for w in t if w not in sw]\n",
    "    \n",
    "    if stemming == None:\n",
    "        pass;\n",
    "    elif stemming == \"Porter\":\n",
    "        t = [ps.stem(w) for w in t]\n",
    "    elif stemming == \"Lancaster\":\n",
    "        t = [ls.stem(w) for w in t]\n",
    "    else:\n",
    "        print(\"Please enter a valid stemming type\")\n",
    "        \n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "\n",
    "    return ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_cleaning_cols(df):\n",
    "    '''\n",
    "    This function generates features and adds them to the data frame.\n",
    "    \n",
    "    Input:\n",
    "        Data frame with raw text strings.\n",
    "        \n",
    "    Output:\n",
    "        Data frame with added columns:\n",
    "            (1) 'split' - (list) Transforms the string of text into a list of words\n",
    "            (2) 'cleaned_w_stopwords' - (string) A string of text where words have been lowercased, \n",
    "                                        punctuation is removed, and stop words are removed\n",
    "            (3) 'cleaned_no_stem' - (string) A string of text where words have been lowercased, and \n",
    "                                        punctuation is removed (stop words remain in text).\n",
    "                                        \n",
    "            \n",
    "            (4) 'cleaned_porter' - (string) A string of text where words have been stemmed using the \n",
    "                                        Porter method on cleaned (lowercase, remove punctuation) text. \n",
    "            (5) 'cleaned_lancaster' - (string) A string of text where words have been stemmed using the\n",
    "                                        Lancaster method on cleaned (lowercase, remove punctuation) text.\n",
    "            (6) 'perc_upper' - (float) Percent of uppercase letters in the string of text.\n",
    "            (7) 'num_exclam' - (integer) Number of times an exclamation point appears in text.\n",
    "            (8) 'num_words' - (integer) Number of words in text.\n",
    "            \n",
    "    '''\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,False),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(\"Porter\",),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(\"Lancaster\",),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    \n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    print(\"DONE\")\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-27 19:52:24.841432\n",
      "2019-05-27 19:52:29.656106\n",
      "2019-05-27 19:55:03.282368\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "add_text_cleaning_cols(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count',\n",
       "       'toxicity_category', 'split', 'cleaned_w_stopwords', 'cleaned_no_stem',\n",
       "       'cleaned_porter', 'cleaned_lancaster', 'perc_upper', 'num_exclam',\n",
       "       'num_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataset and send to s3 bucket:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "pickle_buffer = io.BytesIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket='advancedml-koch-mathur-hinkson'\n",
    "key='preprocessed_train_sample_50pct.pkl'\n",
    "\n",
    "# test.to_pickle(pickle_buffer)\n",
    "# s3_resource.Object(bucket, 'full_preprocessed_test.pkl').put(Body=pickle_buffer.getvalue())\n",
    "\n",
    "test.to_pickle(key)\n",
    "s3_resource.Object(bucket,key).put(Body=open(key, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_sample[train_sample.toxicity_category == 1]\n",
    "nontoxic = train_sample[train_sample.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((144385, 54), (8640, 54), (135745, 54))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the dataset to be include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter = len(toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df = train_sample.sample(quarter*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    25920\n",
      "1     8640\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    17280\n",
      "0    17280\n",
      "Name: toxicity_category, dtype: int64\n",
      "1    25920\n",
      "0     8640\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_25 = toxic.append(nontoxic.sample(len(toxic)*3))\n",
    "prepared_25 = prepared_25.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_25.toxicity_category.value_counts())\n",
    "\n",
    "prepared_50 = toxic.append(toxic).append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_50 = prepared_50.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_50.toxicity_category.value_counts())\n",
    "\n",
    "prepared_75 = toxic.append(toxic).append(toxic).append(nontoxic.sample(len(toxic)))\n",
    "prepared_75 = prepared_75.sample(frac=1).reset_index(drop=True)\n",
    "print(prepared_75.toxicity_category.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, train_perc=.80, addtl_feats =[''], model_type = \"Multi\", \n",
    "             see_inside=False, comments=\"comment_text\",\n",
    "             target='toxicity_category'):\n",
    "    '''\n",
    "    This function runs a single machine learning model as per the specified parameters.\n",
    "    \n",
    "    Input(s):\n",
    "        model_df: source data frame\n",
    "        train_perc: percentage that should be used for training set\n",
    "        addtl_feats: (list) list of non text columns to include\n",
    "        model_type: which machine learning model to use\n",
    "        see_inside: returns the intermediate tokenized and vectorized arrays\n",
    "        comments: source column for text data\n",
    "        target: source column for y values\n",
    "        \n",
    "    Output(s):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    train_start = 0\n",
    "    train_end = round(model_df.shape[0]*train_perc) \n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = model_df.shape[0]\n",
    "    \n",
    "    X_all = model_df[comments].values\n",
    "    y_all = model_df[target].values\n",
    "\n",
    "    # calculating frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    fitted_vectorizer=tfidf_vectorizer.fit(model_df[comments])\n",
    "    X_all_tfidf =  fitted_vectorizer.transform(model_df[comments])\n",
    "    \n",
    "    \n",
    "    X_train = X_all_tfidf[train_start:train_end]\n",
    "    y_train = model_df[train_start:train_end][target].values\n",
    "    y_train=y_train.astype('int')\n",
    "    \n",
    "\n",
    "    X_test = X_all_tfidf[test_start:test_end]\n",
    "    y_test = model_df[test_start:test_end][target].values\n",
    "    \n",
    "    \n",
    "    model_dict = {}\n",
    "    model_dict[\"Mutli\"] = MultinomialNB()\n",
    "    model_dict[\"Gauss\"] = GaussianNB()\n",
    "    model_dict['SVM'] = svm.SVC(kernel='linear', probability=True, random_state=1008)\n",
    "    model_dict[\"LR\"] = LogisticRegression(penalty=\"l1\",C=1e5)\n",
    "        \n",
    "    clf = model_dict[model_type].fit(X_train, y_train)\n",
    "    \n",
    "    predicted = clf.predict(X_test)\n",
    "    \n",
    "    output = model_df[test_start:test_end]\n",
    "    output['predicted'] = predicted\n",
    "    output['y_test'] = y_test\n",
    "    output['accuracy'] = output.predicted == output.y_test\n",
    "    \n",
    "    if see_inside == True:\n",
    "        return clf, output, X_all_counts, X_all_tfidf\n",
    "    else:\n",
    "        return clf, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(output, should_print=True, round_to=3):\n",
    "    metrics = {}\n",
    "    targets = output[output.y_test == 1]\n",
    "    nontargets = output[output.y_test == 0]\n",
    "    \n",
    "    dfs = [output, target, nontarget]\n",
    "    label = [\"Overall\", \"Target\", \"Non-Target\"]\n",
    "    \n",
    "    for i in range(len(dfs)):\n",
    "        \n",
    "        df, label = dfs[i], label[i]\n",
    "        \n",
    "        metrics[label] = {}\n",
    "        \n",
    "        accuracy = round(accuracy_score(y_test, predicted), round_to)\n",
    "        metrics[label]['Accuracy'] = accuracy\n",
    "        \n",
    "        precision = round(precision_score(y_test, predicted), round_to)\n",
    "        metrics[label]['Precision'] = precision\n",
    "\n",
    "        recall = round(recall_score(y_test, predicted), round_to)\n",
    "        metrics[label]['Recall'] = recall\n",
    "        \n",
    "        f1 = round(f1_score(y_test, predicted), round_to)\n",
    "        metrics[label]['F1'] = f1\n",
    "\n",
    "        if label == \"Overall\":\n",
    "            roc_auc = round(roc_auc_score(y_test, predicted), round_to)\n",
    "            metrics[label]['ROC_AUC'] = roc_auc\n",
    "            \n",
    "        if should_print == True:\n",
    "            print(\"{} Accuracy: {}\".format(label, accuracy))\n",
    "            print(\"{} Precision: {}\".format(label, precision))\n",
    "            print(\"{} Recall: {}\".format(label, recall))\n",
    "            if label == \"Overall\":\n",
    "                print(\"ROC_AUC: {}\".format(label, roc_auc))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17092, 25851)\n",
      "The unique values predicted in the training set include :[0 1]\n",
      "The unique values predicted in the test set include :[0 1]\n"
     ]
    }
   ],
   "source": [
    "clf, output = run_model(prepared_df, comments = \"cleaned_lancaster\", should_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_w_stopwords</th>\n",
       "      <th>cleaned_no_stem</th>\n",
       "      <th>cleaned_porter</th>\n",
       "      <th>cleaned_lancaster</th>\n",
       "      <th>perc_upper</th>\n",
       "      <th>num_exclam</th>\n",
       "      <th>num_words</th>\n",
       "      <th>predicted</th>\n",
       "      <th>y_test</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13674</th>\n",
       "      <td>5293834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Preeeecisely.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>preeeecisely</td>\n",
       "      <td>preeeecisely</td>\n",
       "      <td>preeeecisely</td>\n",
       "      <td>preeeecisely</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13675</th>\n",
       "      <td>6043087</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>tol·er·ance\\nˈtäl(ə)rəns/Submit\\nnoun\\n1.\\nthe...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...</td>\n",
       "      <td>tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...</td>\n",
       "      <td>tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nth ...</td>\n",
       "      <td>tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13676</th>\n",
       "      <td>5415503</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>Oh sure, put a black guy in the role of \"Caesa...</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.014493</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>oh sure put a black guy in the role of caesar ...</td>\n",
       "      <td>oh sure put black guy role caesar youd crying ...</td>\n",
       "      <td>oh sure put black guy role caesar youd cri any...</td>\n",
       "      <td>oh sure put black guy rol caesar youd cry anyo...</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13677</th>\n",
       "      <td>5936191</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>Great work getting more scum off the streets.</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>great work getting more scum off the streets</td>\n",
       "      <td>great work getting scum streets</td>\n",
       "      <td>great work get scum streets</td>\n",
       "      <td>gre work get scum streets</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13678</th>\n",
       "      <td>744227</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>There is an error there..  Something U can not...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>there is an error there  something u can not s...</td>\n",
       "      <td>error there something u see smart</td>\n",
       "      <td>error there someth u see smart</td>\n",
       "      <td>er there someth u see smart</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    target                                       comment_text  \\\n",
       "13674  5293834  0.000000                                      Preeeecisely.   \n",
       "13675  6043087  0.166667  tol·er·ance\\nˈtäl(ə)rəns/Submit\\nnoun\\n1.\\nthe...   \n",
       "13676  5415503  0.550725  Oh sure, put a black guy in the role of \"Caesa...   \n",
       "13677  5936191  0.685714      Great work getting more scum off the streets.   \n",
       "13678   744227  0.800000  There is an error there..  Something U can not...   \n",
       "\n",
       "       severe_toxicity   obscene  identity_attack    insult    threat  asian  \\\n",
       "13674         0.000000  0.000000         0.000000  0.000000  0.000000    0.0   \n",
       "13675         0.000000  0.000000         0.000000  0.000000  0.000000    NaN   \n",
       "13676         0.014493  0.014493         0.565217  0.246377  0.000000    0.0   \n",
       "13677         0.071429  0.114286         0.014286  0.628571  0.014286    NaN   \n",
       "13678         0.000000  0.000000         0.000000  0.800000  0.000000    NaN   \n",
       "\n",
       "       atheist  ...                                cleaned_w_stopwords  \\\n",
       "13674      0.0  ...                                       preeeecisely   \n",
       "13675      NaN  ...  tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...   \n",
       "13676      0.0  ...  oh sure put a black guy in the role of caesar ...   \n",
       "13677      NaN  ...       great work getting more scum off the streets   \n",
       "13678      NaN  ...  there is an error there  something u can not s...   \n",
       "\n",
       "                                         cleaned_no_stem  \\\n",
       "13674                                       preeeecisely   \n",
       "13675  tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...   \n",
       "13676  oh sure put black guy role caesar youd crying ...   \n",
       "13677                    great work getting scum streets   \n",
       "13678                  error there something u see smart   \n",
       "\n",
       "                                          cleaned_porter  \\\n",
       "13674                                       preeeecisely   \n",
       "13675  tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nth ...   \n",
       "13676  oh sure put black guy role caesar youd cri any...   \n",
       "13677                        great work get scum streets   \n",
       "13678                     error there someth u see smart   \n",
       "\n",
       "                                       cleaned_lancaster  perc_upper  \\\n",
       "13674                                       preeeecisely       0.077   \n",
       "13675  tol·er·ance\\nˈtäl(ə)rəns/submit\\nnoun\\n1.\\nthe...       0.012   \n",
       "13676  oh sure put black guy rol caesar youd cry anyo...       0.023   \n",
       "13677                          gre work get scum streets       0.022   \n",
       "13678                        er there someth u see smart       0.038   \n",
       "\n",
       "       num_exclam  num_words  predicted  y_test  accuracy  \n",
       "13674           0          1          1       0     False  \n",
       "13675           0         67          0       0      True  \n",
       "13676           0         19          1       1      True  \n",
       "13677           0          8          1       1      True  \n",
       "13678           0         16          0       1     False  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_w_stopwords', 0.6]\n",
      "(17092, 30051)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8221442152991078 , Target Accuracy: 0.8695023148148148, Nontarget Accuracy: 0.7737355811889973\n",
      "\n",
      "['cleaned_w_stopwords', 0.7]\n",
      "(17092, 30051)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8260530421216848 , Target Accuracy: 0.8893617021276595, Nontarget Accuracy: 0.7616987809673614\n",
      "\n",
      "['cleaned_w_stopwords', 0.8]\n",
      "(17092, 30051)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8224107665301346 , Target Accuracy: 0.9002347417840375, Nontarget Accuracy: 0.7450408401400234\n",
      "\n",
      "['cleaned_no_stem', 0.6]\n",
      "(17092, 30048)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8206815854907124 , Target Accuracy: 0.8778935185185185, Nontarget Accuracy: 0.7622005323868678\n",
      "\n",
      "['cleaned_no_stem', 0.7]\n",
      "(17092, 30048)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8229329173166927 , Target Accuracy: 0.8924564796905222, Nontarget Accuracy: 0.7522611089264648\n",
      "\n",
      "['cleaned_no_stem', 0.8]\n",
      "(17092, 30048)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.823288472791106 , Target Accuracy: 0.9019953051643192, Nontarget Accuracy: 0.7450408401400234\n",
      "\n",
      "['cleaned_porter', 0.6]\n",
      "(17092, 27260)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8157086441421676 , Target Accuracy: 0.8674768518518519, Nontarget Accuracy: 0.7627920733510796\n",
      "\n",
      "['cleaned_porter', 0.7]\n",
      "(17092, 27260)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8155226209048362 , Target Accuracy: 0.8796905222437137, Nontarget Accuracy: 0.750294927251278\n",
      "\n",
      "['cleaned_porter', 0.8]\n",
      "(17092, 27260)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8115857226448215 , Target Accuracy: 0.8937793427230047, Nontarget Accuracy: 0.7298716452742123\n",
      "\n",
      "['cleaned_lancaster', 0.6]\n",
      "(17092, 25851)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8174637999122422 , Target Accuracy: 0.8666087962962963, Nontarget Accuracy: 0.7672286305826679\n",
      "\n",
      "['cleaned_lancaster', 0.7]\n",
      "(17092, 25851)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8207878315132605 , Target Accuracy: 0.881237911025145, Nontarget Accuracy: 0.7593393629571372\n",
      "\n",
      "['cleaned_lancaster', 0.8]\n",
      "(17092, 25851)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8089526038619076 , Target Accuracy: 0.8879107981220657, Nontarget Accuracy: 0.7304550758459744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = 0\n",
    "metric_dict = ''\n",
    "model_factors = []\n",
    "\n",
    "SUBSET_OF_INTEREST = \"Target\"\n",
    "METRIC_OF_INTEREST = \"F1\"\n",
    "\n",
    "dfs = [random_df, prepared_25, prepared_50, prepared_75]\n",
    "label = [\"random_df\", \"prepared_25\", \"prepared_50\", \"prepared_75\"]\n",
    "\n",
    "for i in range(len(dfs)):\n",
    "    for text in ['cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter', 'cleaned_lancaster']:\n",
    "\n",
    "        factors = [label[i], text, tp]\n",
    "        print(factors)\n",
    "\n",
    "        clf, output = run_model(dfs[i], comments = text, should_print=False)\n",
    "        metrics = get_metrics(model)\n",
    "        metric_of_interest = metrics[SUBSET_OF_INTEREST][METRIC_OF_INTEREST]\n",
    "\n",
    "        if metric_of_intest > best_metric:\n",
    "            best_metric = metric_of_interest\n",
    "            \n",
    "            model_factors = factors\n",
    "            metric_dict = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9019953051643192, ['cleaned_no_stem', 0.8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_factors, best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17092, 30048)\n"
     ]
    }
   ],
   "source": [
    "metric_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
