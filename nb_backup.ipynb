{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "import datetime\n",
    "\n",
    "import s3fs\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in test.csv and train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column called \"toxicity_category\" in the train data frame categorizing comments as toxic (\"1\") or non-toxic (\"0\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['toxicity_category'] = train.target.apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train.csv into training (80%) and validation sets (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(train)) < 0.8\n",
    "train_set = train[msk]\n",
    "validation_set = train[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1359168\n",
      "1      85107\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    339268\n",
      "1     21331\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validation_set.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small sample (\"train_sample1\") from the train_set on which to run models.  Ensure that samples are iid by replacing after each draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample1 = train_set.sample(frac=0.05, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    67882\n",
      "1     4332\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_sample1.toxicity_category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()\n",
    "ps = PorterStemmer() \n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "sw.add('')\n",
    "\n",
    "def clean_text(text, stemming=None, remove_sw = True):\n",
    "    '''\n",
    "    This auxiliary function cleans text.\n",
    "    \n",
    "    Methods used for cleaning are: \n",
    "        (1) transform string of text to list of words,\n",
    "        (2) cleaned (lowercase, remove punctuation) and remove stop words,\n",
    "        (3) Porter stemming of cleaned (lowercase, remove punctuation) text, \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    \n",
    "    Inputs:\n",
    "        text (string) - A string of text.\n",
    "        stemming (parameter) - either Porter or Lancaster stemming method\n",
    "        remove_sw (boolean) - True/False remove stop words\n",
    "    \n",
    "    Outputs:\n",
    "        Cleaned text per the input parameters.\n",
    "    '''\n",
    "\n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    \n",
    "    t = [w.lower() for w in t]\n",
    "    \n",
    "    if remove_sw == True:\n",
    "        t = [w for w in t if w not in sw]\n",
    "    \n",
    "    if stemming == None:\n",
    "        pass;\n",
    "    elif stemming == \"Porter\":\n",
    "        t = [ps.stem(w) for w in t]\n",
    "    elif stemming == \"Lancaster\":\n",
    "        t = [ls.stem(w) for w in t]\n",
    "    else:\n",
    "        print(\"Please enter a valid stemming type\")\n",
    "        \n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "\n",
    "    return ' '.join(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_cleaning_cols(df):\n",
    "    '''\n",
    "    This function generates features.\n",
    "    \n",
    "    Input:\n",
    "        dataframe with raw text strings\n",
    "        \n",
    "    Output:\n",
    "        dataframe with added columns:\n",
    "            (1) 'split' - (list) Transforms the string of text into a list of words\n",
    "            (2) 'cleaned_w_stopwords' - (string) A string of text where words have been lowercased, \n",
    "                                        punctuation is removed, and stop words are removed\n",
    "            (3) 'cleaned_no_stem' - (string) A string of text where words have been lowercased, and \n",
    "                                        punctuation is removed (stop words remain in text).\n",
    "                                        \n",
    "            \n",
    "            (4) 'cleaned_porter' - A striPorter stemming of cleaned (lowercase, remove punctuation) text. \n",
    "        (4) Lancaster stemming of cleaned (lowercase, remove punctuation), \n",
    "        (5) cleaned (lowercase, remove punctuation) without removing stop words.\n",
    "    '''\n",
    "    print(datetime.datetime.now())\n",
    "    \n",
    "    df['split'] = df[\"comment_text\"].apply(lambda x: x.split(\" \"))\n",
    "    df['cleaned_w_stopwords'] = df[\"comment_text\"].apply(clean_text,args=(None,False),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "    df['cleaned_no_stem'] = df[\"comment_text\"].apply(clean_text,)\n",
    "    df['cleaned_porter'] = df[\"comment_text\"].apply(clean_text,args=(\"Porter\",),)\n",
    "    df['cleaned_lancaster'] = df[\"comment_text\"].apply(clean_text,args=(\"Lancaster\",),)\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_upper'] = df[\"comment_text\"].apply(lambda x: round((len(re.findall(r'[A-Z]',x)) / len(x)), 3))\n",
    "\n",
    "    df['num_exclam'] = df[\"comment_text\"].apply(lambda x:(len(re.findall(r'!',x))))\n",
    "    \n",
    "    df['num_words'] = df[\"split\"].apply(lambda x: len(x))\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    df['perc_stopwords'] = round((df.num_words - df['cleaned_no_stem'].apply(lambda x: len(x)))/df.num_words,3) \n",
    "    \n",
    "    df['num_upper_words'] = df[\"split\"].apply(lambda x: sum(map(str.isupper, x)))\n",
    "    \n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-26 18:57:36.255069\n",
      "2019-05-26 18:57:38.055402\n",
      "2019-05-26 18:58:36.011581\n",
      "2019-05-26 18:58:36.478395\n",
      "2019-05-26 18:58:36.803464\n"
     ]
    }
   ],
   "source": [
    "add_text_cleaning_cols(train_sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count',\n",
       "       'category_toxicity', 'toxicity_category', 'split',\n",
       "       'cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter',\n",
       "       'cleaned_lancaster', 'perc_upper', 'num_exclam', 'num_words',\n",
       "       'perc_stopwords', 'num_upper_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle the dataset and send to s3 bucket:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "pickle_buffer = io.BytesIO()\n",
    "s3_resource = boto3.resource('s3')\n",
    "bucket='advancedml-koch-mathur-hinkson'\n",
    "key='preprocessed_train_sample_50pct.pkl'\n",
    "\n",
    "# test.to_pickle(pickle_buffer)\n",
    "# s3_resource.Object(bucket, 'full_preprocessed_test.pkl').put(Body=pickle_buffer.getvalue())\n",
    "\n",
    "test.to_pickle(key)\n",
    "s3_resource.Object(bucket,key).put(Body=open(key, 'rb'))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "addtl_cols = ['perc_upper', 'num_exclam', 'num_words', 'perc_stopwords', 'num_upper_words']\n",
    "#addtl_cols = ['perc_upper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72214, 57)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>split</th>\n",
       "      <th>cleaned_w_stopwords</th>\n",
       "      <th>cleaned_no_stem</th>\n",
       "      <th>cleaned_porter</th>\n",
       "      <th>cleaned_lancaster</th>\n",
       "      <th>perc_upper</th>\n",
       "      <th>num_exclam</th>\n",
       "      <th>num_words</th>\n",
       "      <th>perc_stopwords</th>\n",
       "      <th>num_upper_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>375600</th>\n",
       "      <td>702857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>You completely misstate how the  Bulletin of t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[You, completely, misstate, how, the, , Bullet...</td>\n",
       "      <td>you completely misstate how the  bulletin of t...</td>\n",
       "      <td>completely misstate bulletin atomic scientists...</td>\n",
       "      <td>complet misstat bulletin atom scientist move d...</td>\n",
       "      <td>complet misst bulletin atom sci mov doomsday c...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>-2.690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566597</th>\n",
       "      <td>6038615</td>\n",
       "      <td>0.3</td>\n",
       "      <td>The good thing about boondoggles is that they ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[The, good, thing, about, boondoggles, is, tha...</td>\n",
       "      <td>the good thing about boondoggles is that they ...</td>\n",
       "      <td>good thing boondoggles usually collapse weight...</td>\n",
       "      <td>good thing boondoggl usual collaps weight corr...</td>\n",
       "      <td>good thing boondoggl us collaps weight corruption</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>-2.353</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275683</th>\n",
       "      <td>579993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Throwin your votes away eh</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[Throwin, your, votes, away, eh]</td>\n",
       "      <td>throwin your votes away eh</td>\n",
       "      <td>throwin votes away eh</td>\n",
       "      <td>throwin vote away eh</td>\n",
       "      <td>throwin vot away eh</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89607</th>\n",
       "      <td>352281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mr. Sayre trunk line fibers typically carry 40...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[Mr., Sayre, trunk, line, fibers, typically, c...</td>\n",
       "      <td>mr sayre trunk line fibers typically carry 40 ...</td>\n",
       "      <td>mr sayre trunk line fibers typically carry 40 ...</td>\n",
       "      <td>mr sayr trunk line fiber typic carri 40 separ ...</td>\n",
       "      <td>mr sayr trunk lin fib typ carry 40 sep 10 gb/s...</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>-2.902</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347516</th>\n",
       "      <td>5762238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MSNBC and CNN often mention facts  ..............</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[MSNBC, and, CNN, often, mention, facts, , ......</td>\n",
       "      <td>msnbc and cnn often mention facts   \\n althoug...</td>\n",
       "      <td>msnbc cnn often mention facts  \\n although sto...</td>\n",
       "      <td>msnbc cnn often mention fact  \\n although stop...</td>\n",
       "      <td>msnbc cnn oft ment fact  \\n although stop watc...</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>-3.833</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  target                                       comment_text  \\\n",
       "375600    702857     0.0  You completely misstate how the  Bulletin of t...   \n",
       "1566597  6038615     0.3  The good thing about boondoggles is that they ...   \n",
       "275683    579993     0.0                         Throwin your votes away eh   \n",
       "89607     352281     0.0  Mr. Sayre trunk line fibers typically carry 40...   \n",
       "1347516  5762238     0.0  MSNBC and CNN often mention facts  ..............   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "375600               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "1566597              0.0      0.0              0.0     0.3     0.0    NaN   \n",
       "275683               0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "89607                0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "1347516              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "         atheist  ...                                              split  \\\n",
       "375600       NaN  ...  [You, completely, misstate, how, the, , Bullet...   \n",
       "1566597      NaN  ...  [The, good, thing, about, boondoggles, is, tha...   \n",
       "275683       NaN  ...                   [Throwin, your, votes, away, eh]   \n",
       "89607        NaN  ...  [Mr., Sayre, trunk, line, fibers, typically, c...   \n",
       "1347516      NaN  ...  [MSNBC, and, CNN, often, mention, facts, , ......   \n",
       "\n",
       "                                       cleaned_w_stopwords  \\\n",
       "375600   you completely misstate how the  bulletin of t...   \n",
       "1566597  the good thing about boondoggles is that they ...   \n",
       "275683                          throwin your votes away eh   \n",
       "89607    mr sayre trunk line fibers typically carry 40 ...   \n",
       "1347516  msnbc and cnn often mention facts   \\n althoug...   \n",
       "\n",
       "                                           cleaned_no_stem  \\\n",
       "375600   completely misstate bulletin atomic scientists...   \n",
       "1566597  good thing boondoggles usually collapse weight...   \n",
       "275683                               throwin votes away eh   \n",
       "89607    mr sayre trunk line fibers typically carry 40 ...   \n",
       "1347516  msnbc cnn often mention facts  \\n although sto...   \n",
       "\n",
       "                                            cleaned_porter  \\\n",
       "375600   complet misstat bulletin atom scientist move d...   \n",
       "1566597  good thing boondoggl usual collaps weight corr...   \n",
       "275683                                throwin vote away eh   \n",
       "89607    mr sayr trunk line fiber typic carri 40 separ ...   \n",
       "1347516  msnbc cnn often mention fact  \\n although stop...   \n",
       "\n",
       "                                         cleaned_lancaster  perc_upper  \\\n",
       "375600   complet misst bulletin atom sci mov doomsday c...       0.030   \n",
       "1566597  good thing boondoggl us collaps weight corruption       0.010   \n",
       "275683                                 throwin vot away eh       0.038   \n",
       "89607    mr sayr trunk lin fib typ carry 40 sep 10 gb/s...       0.025   \n",
       "1347516  msnbc cnn oft ment fact  \\n although stop watc...       0.085   \n",
       "\n",
       "         num_exclam  num_words  perc_stopwords  num_upper_words  \n",
       "375600            0         29          -2.690                0  \n",
       "1566597           0         17          -2.353                0  \n",
       "275683            0          5          -3.200                0  \n",
       "89607             0        183          -2.902                3  \n",
       "1347516           0         24          -3.833                3  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to memory issues, we needed to use a smaller training set.  We used a random iid sample of half of the train_sample1 frame to train NB model:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "subset_train_sample1 = train_sample1.sample(frac=0.5, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = train_sample1[train_sample1.toxicity_category == 1]\n",
    "nontoxic = train_sample1[train_sample1.toxicity_category == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((72214, 57), (4332, 57), (67882, 57))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample1.shape, toxic.shape, nontoxic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping the dataset to be include an equal number of toxic and nontoxic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    8664\n",
      "0    8664\n",
      "Name: toxicity_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "prepared_df = toxic.append(toxic).append(nontoxic.sample(len(toxic)*2))\n",
    "prepared_df = prepared_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(prepared_df.toxicity_category.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are unable to train an NB model on categorical (text) and continuous (numerical) data at the same time, our action plan changed to running two independent models for each type of data and then running a thrid NB model on the resulting predict_proba from the other two trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_df, train_perc=.80, addtl_feats =[''], model_type = \"Multi\", \n",
    "              num_iter = 10, should_print=False, see_inside=False, comments=\"comment_text\",\n",
    "             target='toxicity_category'):\n",
    "    \n",
    "    train_start = 0\n",
    "    train_end = round(model_df.shape[0]*train_perc) \n",
    "\n",
    "    test_start = train_end\n",
    "    test_end = model_df.shape[0]\n",
    "    \n",
    "    X_all = model_df[comments].values\n",
    "    y_all = model_df[target].values\n",
    "    \n",
    "    # tokenizing text\n",
    "#     count_vect = CountVectorizer()\n",
    "#     X_all_counts = count_vect.fit_transform(X_all.astype('U'))\n",
    "    #print(X_all_counts.shape)\n",
    "\n",
    "    # calculating frequencies\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    fitted_vectorizer=tfidf_vectorizer.fit(model_df[comments])\n",
    "    X_all_tfidf =  fitted_vectorizer.transform(model_df[comments])\n",
    "\n",
    "\n",
    "    print(X_all_tfidf.shape)\n",
    "    \n",
    "    if addtl_feats != ['']: # combine non-text and text features if necessary\n",
    "        print(\"here\")\n",
    "#         others_all = model_df[addtl_feats].values.reshape(-1,1)\n",
    "\n",
    "        others_all = model_df[addtl_feats].values.reshape(-1,len(addtl_feats))\n",
    "        #print(others_all)\n",
    "        newfeatures_all = sparse.hstack((X_all_tfidf, others_all.astype(float))).tocsr()\n",
    "    else:\n",
    "        newfeatures_all = X_all_tfidf\n",
    "    \n",
    "    \n",
    "    X_train = newfeatures_all[train_start:train_end]\n",
    "    y_train = model_df[train_start:train_end][target].values\n",
    "    y_train=y_train.astype('int')\n",
    "    \n",
    "\n",
    "    X_test = newfeatures_all[test_start:test_end]\n",
    "    y_test = model_df[test_start:test_end][target].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if model_type == 'Multi':\n",
    "        clf = MultinomialNB().fit(X_train, y_train)\n",
    "    if model_type == \"Gauss\":\n",
    "        clf = GaussianNB().fit(X_train, y_train) \n",
    "    if model_type == \"SVM\":\n",
    "        clf = svm.SVC(kernel='linear', probability=True, random_state=1008).fit(X_train, y_train) \n",
    "        \n",
    "    preds_for_train = clf.predict(X_train)\n",
    "    \n",
    "    \n",
    "   \n",
    "    predicted = clf.predict(X_test)\n",
    "    accuracy = np.mean(predicted == y_test)\n",
    "    \n",
    "    output = model_df[test_start:test_end]\n",
    "    output['predicted'] = predicted\n",
    "    output['y_test'] = y_test\n",
    "    output['accuracy'] = output.predicted == output.y_test\n",
    "    \n",
    "\n",
    "#     y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "#     precision = precision_score(y_true_sorted, preds)\n",
    "\n",
    "\n",
    "    if should_print == True:\n",
    "\n",
    "        print(\"The accuracy on the test set is {}%.\".format(round(accuracy*100,2)))    \n",
    "    \n",
    "    if see_inside == True:\n",
    "        return clf, accuracy, X_all_counts, X_all_tfidf\n",
    "    else:\n",
    "        return clf, accuracy, preds_for_train, predicted, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17328, 25803)\n",
      "The unique values predicted in the training set include :[0 1]\n",
      "The unique values predicted in the test set include :[0 1]\n"
     ]
    }
   ],
   "source": [
    "clf1, accuracy, preds_for_train, predicted , output = run_model(prepared_df, comments = \"cleaned_lancaster\", should_print=False)\n",
    "\n",
    "print(\"The unique values predicted in the training set include :\" + str(np.unique(preds_for_train)))\n",
    "print(\"The unique values predicted in the test set include :\" + str(np.unique(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_porter</th>\n",
       "      <th>cleaned_lancaster</th>\n",
       "      <th>perc_upper</th>\n",
       "      <th>num_exclam</th>\n",
       "      <th>num_words</th>\n",
       "      <th>perc_stopwords</th>\n",
       "      <th>num_upper_words</th>\n",
       "      <th>predicted</th>\n",
       "      <th>y_test</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13862</th>\n",
       "      <td>5047544</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>Anne McLellan a true waste of space then and n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ann mclellan true wast space like ilk</td>\n",
       "      <td>an mclellan tru wast spac lik ilk</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>-1.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13863</th>\n",
       "      <td>1082637</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Thieves.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>thieves</td>\n",
       "      <td>thieves</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13864</th>\n",
       "      <td>802459</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>The third world can't ever become the first wo...</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>third world can't ever becom first world keep ...</td>\n",
       "      <td>third world can't ev becom first world keep tr...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>-3.333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13865</th>\n",
       "      <td>1072184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Well, your post responded to a comment about t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>well post respond comment this everi stori i'v...</td>\n",
       "      <td>well post respond com this every story i've re...</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>-2.324</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13866</th>\n",
       "      <td>5180644</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>This guy is an idiot. He was all in for shorti...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>guy idiot short canadian bank probabl took man...</td>\n",
       "      <td>guy idiot short canad bank prob took many road...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>-1.971</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    target                                       comment_text  \\\n",
       "13862  5047544  0.226415  Anne McLellan a true waste of space then and n...   \n",
       "13863  1082637  0.600000                                           Thieves.   \n",
       "13864   802459  0.555556  The third world can't ever become the first wo...   \n",
       "13865  1072184  0.000000  Well, your post responded to a comment about t...   \n",
       "13866  5180644  0.903226  This guy is an idiot. He was all in for shorti...   \n",
       "\n",
       "       severe_toxicity   obscene  identity_attack    insult    threat  asian  \\\n",
       "13862         0.000000  0.000000         0.000000  0.245283  0.000000    NaN   \n",
       "13863         0.000000  0.100000         0.100000  0.600000  0.000000    NaN   \n",
       "13864         0.041667  0.111111         0.027778  0.527778  0.000000    NaN   \n",
       "13865         0.000000  0.000000         0.000000  0.000000  0.000000    0.0   \n",
       "13866         0.000000  0.193548         0.048387  0.887097  0.016129    NaN   \n",
       "\n",
       "       atheist  ...                                     cleaned_porter  \\\n",
       "13862      NaN  ...              ann mclellan true wast space like ilk   \n",
       "13863      NaN  ...                                            thieves   \n",
       "13864      NaN  ...  third world can't ever becom first world keep ...   \n",
       "13865      0.0  ...  well post respond comment this everi stori i'v...   \n",
       "13866      NaN  ...  guy idiot short canadian bank probabl took man...   \n",
       "\n",
       "                                       cleaned_lancaster  perc_upper  \\\n",
       "13862                  an mclellan tru wast spac lik ilk       0.043   \n",
       "13863                                            thieves       0.125   \n",
       "13864  third world can't ev becom first world keep tr...       0.016   \n",
       "13865  well post respond com this every story i've re...       0.055   \n",
       "13866  guy idiot short canad bank prob took many road...       0.028   \n",
       "\n",
       "       num_exclam  num_words  perc_stopwords  num_upper_words  predicted  \\\n",
       "13862           0         15          -1.600                0          1   \n",
       "13863           0          1          -6.000                0          1   \n",
       "13864           0         87          -3.333                0          0   \n",
       "13865           0         37          -2.324                2          0   \n",
       "13866           0         35          -1.971                0          1   \n",
       "\n",
       "       y_test  accuracy  \n",
       "13862       0     False  \n",
       "13863       1      True  \n",
       "13864       1     False  \n",
       "13865       0      True  \n",
       "13866       1      True  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1735\n",
       "0    1731\n",
       "Name: y_test, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2060\n",
       "0    1406\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.predicted.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.811598384304674"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9054755043227666"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = output[output.y_test == 1]\n",
    "targets[targets.accuracy == True].shape[0] / targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     1242\n",
       "False     489\n",
       "Name: accuracy, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[output.y_test == 0].accuracy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_w_stopwords', 0.6]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8094070119751839 , Target Accuracy: 0.8924050632911392\n",
      "\n",
      "['cleaned_w_stopwords', 0.7]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.812043093497499 , Target Accuracy: 0.9043377226955848\n",
      "\n",
      "['cleaned_w_stopwords', 0.8]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8228505481823427 , Target Accuracy: 0.9135446685878963\n",
      "\n",
      "['cleaned_no_stem', 0.6]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8117154811715481 , Target Accuracy: 0.8975834292289988\n",
      "\n",
      "['cleaned_no_stem', 0.7]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8122354751827626 , Target Accuracy: 0.9024012393493416\n",
      "\n",
      "['cleaned_no_stem', 0.8]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8260242354298903 , Target Accuracy: 0.9175792507204611\n",
      "\n",
      "['cleaned_porter', 0.6]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8030587216851826 , Target Accuracy: 0.8840621403912543\n",
      "\n",
      "['cleaned_porter', 0.7]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8091573682185456 , Target Accuracy: 0.895429899302866\n",
      "\n",
      "['cleaned_porter', 0.8]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8176572417772648 , Target Accuracy: 0.9031700288184438\n",
      "\n",
      "['cleaned_lancaster', 0.6]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8083970567017746 , Target Accuracy: 0.8849252013808976\n",
      "\n",
      "['cleaned_lancaster', 0.7]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8101192766448634 , Target Accuracy: 0.8938807126258714\n",
      "\n",
      "['cleaned_lancaster', 0.8]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.811598384304674 , Target Accuracy: 0.9054755043227666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "model_factors = []\n",
    "\n",
    "for text in ['cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter',\n",
    "    'cleaned_lancaster']:\n",
    "    for tp in [0.6, 0.7, 0.8]:\n",
    "  \n",
    "        factors = [text, tp]\n",
    "        print(factors)\n",
    "\n",
    "        clf, accuracy, preds_for_train, predicted, output = run_model(prepared_df, train_perc = tp, comments = text, should_print=False)\n",
    "        \n",
    "        print(\"The unique values predicted for the training set include :\" + str(np.unique(preds_for_train)))\n",
    "        print(\"The unique values predicted for the test set include :\" + str(np.unique(predicted)))\n",
    "        \n",
    "        targets = output[output.y_test == 1]\n",
    "        target_accuracy = targets[targets.accuracy == True].shape[0] / targets.shape[0]\n",
    "        \n",
    "        print(\"Accuracy: {} , Target Accuracy: {}\".format(accuracy, target_accuracy))\n",
    "\n",
    "        if target_accuracy > best_accuracy:\n",
    "            model_factors = factors\n",
    "            best_accuracy = target_accuracy\n",
    "\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cleaned_w_stopwords', 0.6]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.874909825422017 , Target Accuracy: 0.8627733026467204\n",
      "\n",
      "['cleaned_w_stopwords', 0.7]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8855328972681801 , Target Accuracy: 0.8772269558481797\n",
      "\n",
      "['cleaned_w_stopwords', 0.8]\n",
      "(17328, 30033)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8993075591459896 , Target Accuracy: 0.906628242074928\n",
      "\n",
      "['cleaned_no_stem', 0.6]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.875486942721108 , Target Accuracy: 0.8593210586881473\n",
      "\n",
      "['cleaned_no_stem', 0.7]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8897652943439784 , Target Accuracy: 0.8822618125484121\n",
      "\n",
      "['cleaned_no_stem', 0.8]\n",
      "(17328, 30032)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.9013271783035199 , Target Accuracy: 0.899135446685879\n",
      "\n",
      "['cleaned_porter', 0.6]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8642331553888328 , Target Accuracy: 0.8483889528193326\n",
      "\n",
      "['cleaned_porter', 0.7]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8772604848018468 , Target Accuracy: 0.8690937257939582\n",
      "\n",
      "['cleaned_porter', 0.8]\n",
      "(17328, 27310)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8860357761107905 , Target Accuracy: 0.8853025936599423\n",
      "\n",
      "['cleaned_lancaster', 0.6]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8692829317558793 , Target Accuracy: 0.8555811277330264\n",
      "\n",
      "['cleaned_lancaster', 0.7]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.8768757214313198 , Target Accuracy: 0.8737412858249419\n",
      "\n",
      "['cleaned_lancaster', 0.8]\n",
      "(17328, 25803)\n",
      "The unique values predicted for the training set include :[0 1]\n",
      "The unique values predicted for the test set include :[0 1]\n",
      "Accuracy: 0.881130986728217 , Target Accuracy: 0.8829971181556195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy_svm = 0\n",
    "model_factors_svm = []\n",
    "\n",
    "for text in ['cleaned_w_stopwords', 'cleaned_no_stem', 'cleaned_porter',\n",
    "    'cleaned_lancaster']:\n",
    "    for tp in [0.6, 0.7, 0.8]:\n",
    "  \n",
    "        factors = [text, tp]\n",
    "        print(factors)\n",
    "\n",
    "        clf, accuracy, preds_for_train, predicted, output = run_model(prepared_df, model_type=\"SVM\", train_perc = tp, comments = text, should_print=False)\n",
    "        \n",
    "        print(\"The unique values predicted for the training set include :\" + str(np.unique(preds_for_train)))\n",
    "        print(\"The unique values predicted for the test set include :\" + str(np.unique(predicted)))\n",
    "        \n",
    "        targets = output[output.y_test == 1]\n",
    "        target_accuracy = targets[targets.accuracy == True].shape[0] / targets.shape[0]\n",
    "        \n",
    "        print(\"Accuracy: {} , Target Accuracy: {}\".format(accuracy, target_accuracy))\n",
    "\n",
    "        if target_accuracy > best_accuracy:\n",
    "            model_factors = factors\n",
    "            best_accuracy = target_accuracy\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy_svm, model_factors_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
