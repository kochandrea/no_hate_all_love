{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "b6shNJdp1GuO",
        "2QaIVi9O1daj",
        "gcnufZwcNRJ3",
        "-o8nY8D9OW4n"
      ],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcENPI2NQ1rW",
        "colab_type": "text"
      },
      "source": [
        "# Identifying Hate Speech in Social Media Comments: PyTorch LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4eDhtxDQ7Ey",
        "colab_type": "text"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zx9y7jLffJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import ssl\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import math\n",
        "import string\n",
        "import pickle\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as tud\n",
        "from collections import Counter, OrderedDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8BYwZSkIswo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "warnings.filterwarnings(action='once')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZBI0yauX7w",
        "colab_type": "code",
        "outputId": "72c5abe4-b883-4b8e-b220-da8d5564fb00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKsJchrPuLYE",
        "colab_type": "code",
        "outputId": "b3beed9e-9db4-4414-feb2-f57c0ddc502d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import files, auth\n",
        "auth.authenticate_user()\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/auth.py:158: ResourceWarning: unclosed <ssl.SSLSocket fd=56, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 51438), raddr=('172.217.25.77', 443)>\n",
            "  if _check_adc():\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-614cd509-fa9f-46ea-8291-c8d88441fe1d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-614cd509-fa9f-46ea-8291-c8d88441fe1d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving exec_lstm.py to exec_lstm.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuaHFOJxffJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import exec_lstm as exl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_CtPMsJPe69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLFYlzfkffJ4",
        "colab_type": "code",
        "outputId": "205919ff-ad4c-434d-b68d-06fec6edf496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2CoPnulZEyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import importlib\n",
        "# import google\n",
        "# importlib.reload(google.colab.auth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms0t4XSSHIsw",
        "colab_type": "code",
        "outputId": "a19be195-4665-4090-cfa2-a8213ec57955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/auth.py:141: ResourceWarning: unclosed <ssl.SSLSocket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 56462), raddr=('209.85.145.84', 443)>\n",
            "  if _check_adc():\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnd-yCkXIAAR",
        "colab_type": "code",
        "outputId": "7dc70b88-f610-47fc-9080-253f76625406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "!pip install gcsfs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gcsfs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/19/68ab4e6570a7882698058be8ecf1b195b0b784b838ac1b0ea82c422c0f5a/gcsfs-0.2.2.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 24.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.6/dist-packages (from gcsfs) (1.4.2)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from gcsfs) (0.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gcsfs) (2.21.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from gcsfs) (4.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (1.12.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (0.2.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (4.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->gcsfs) (1.2.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gcsfs) (2019.3.9)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.0.1)\n",
            "Building wheels for collected packages: gcsfs\n",
            "  Building wheel for gcsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/0f/b9/5bc5222756d121ccace51ab3084a1c733380908a4e2f939038\n",
            "Successfully built gcsfs\n",
            "Installing collected packages: gcsfs\n",
            "Successfully installed gcsfs-0.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/gcsfs-0.2.2.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2mbxM4gHnqf",
        "colab_type": "code",
        "outputId": "966498aa-9dc1-4860-914d-84d711b5a588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gcsfs\n",
        "train_full = pd.read_csv('gs://no-hate/train.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOUNQIaffKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_full['label'] = train_full.target.apply(lambda x: 1 if x > 0.5 else 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAS5oWmwffKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ratio = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSmvany6N4BH",
        "colab_type": "text"
      },
      "source": [
        "## Housekeeping and Variable Storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0V-qB-7cpTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ_0Bo0bomng",
        "colab_type": "code",
        "outputId": "ab665b45-fe1a-4236-ac4c-e3787e99ea57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "project_id = 'tribal-monolith-242216'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud alpha survey\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMKCRNboFTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def send_to_bucket(filename, bucket_name=\"no-hate\", directory=None):\n",
        "    if directory:\n",
        "        !gsutil cp /content/{filename} gs://{bucket_name}/{directory}/\n",
        "    else:\n",
        "        !gsutil cp /content/{filename} gs://{bucket_name}/\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86RCZW-1pUOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# send_to_bucket(filename='20pct_imbalanced_model_state.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp4rsgW_sYoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "# import cloudstorage\n",
        "import gcsfs\n",
        "from io import BytesIO\n",
        "import pickle\n",
        "\n",
        "def pickle_to_gcs(df, filename, bucket_name=\"no-hate\", directory=None):\n",
        "    pickle_buffer = io.BytesIO()\n",
        "    fs = gcsfs.GCSFileSystem(project=project_id)\n",
        "    df.to_pickle(filename)\n",
        "    \n",
        "    if directory:\n",
        "        directory = directory + \"/\"\n",
        "    else:\n",
        "        directory = \"\"\n",
        "    \n",
        "    with fs.open(f\"{bucket_name}/{directory}{filename}\", \"wb\") as handle:    \n",
        "        pickle.dump(df, handle)\n",
        "        \n",
        "    \n",
        "def load_pickle_from_gcs(filename, bucket_name=\"no-hate\",  directory=None):\n",
        "    fs = gcsfs.GCSFileSystem(project=project_id) \n",
        "    \n",
        "    if directory:\n",
        "        directory = directory + \"/\"\n",
        "    else:\n",
        "        directory = \"\"\n",
        "        \n",
        "    with fs.open(f\"{bucket_name}/{directory}{filename}\", \"rb\") as handle:\n",
        "        df = pickle.load(handle)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HSe4sRxuDkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pickle_to_gcs(test_set, '20pct_test_set.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gugtkx87zjTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(test_set.shape, test_set.comment_text.isna().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCyoLsY0zDFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check = load_pickle_from_gcs('20pct_test_set.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwzOEfL1zgAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(check.shape, check.comment_text.isna().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIw-KDeBFln9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check Available RAM\n",
        "\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#     process = psutil.Process(os.getpid())\n",
        "#     print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#     print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGRp_AzPF5N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear RAM\n",
        "\n",
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avoL6-pwffKJ",
        "colab_type": "text"
      },
      "source": [
        "## Creating Batch Loader to Enable Larger Dataset Training\n",
        "\n",
        "Use 20% data partition while building batch loader for LSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6shNJdp1GuO",
        "colab_type": "text"
      },
      "source": [
        "### Create Batch Loader Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYmI6n9ZffKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_full = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9SFTFjqffKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, val_set, test_set = exl.get_samples(train_full, proportion=0.2, \n",
        "                                                  train_test_ratio=(1-test_ratio))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHD64__LffKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_sample.shape, val_set.shape, test_set.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZgVpVX2bP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sample.loc[train_sample.comment_text != ''].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gn6znXcffKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abjEy-Zd57Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_drop = train_sample[train_sample.comment_text.apply(exl.tokenizer).apply(len) <= 0].index\n",
        "# train_drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w0GAP6L6P90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_sample = train_sample.drop(train_drop, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQd7VXHlffKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_samples = val_set.sample( n=math.ceil(len(train_sample)*test_ratio), random_state=1008 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJngfdLJffKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_samples.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwD-mIMV53lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_drop = val_samples[val_samples.comment_text.apply(exl.tokenizer).apply(len) <= 0].index\n",
        "# val_drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYWrlBF86eu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_samples = val_samples.drop(val_drop, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMdjtqFwf8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_samples = test_set.sample( n=math.ceil(len(train_sample)*test_ratio), random_state=1008 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC6h9x0w5du4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_drop = test_samples[test_samples.comment_text.apply(exl.tokenizer).apply(len) <= 0].index\n",
        "# test_drop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddbNYd9C6ieq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_samples = test_samples.drop(test_drop, axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhJ-qFHp1RTR",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_u3VHHsffKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# H/T for the torchtext dataframe-based-Dataset hack to: \n",
        "# https://stackoverflow.com/questions/52602071/dataframe-as-datasource-in-torchtext\n",
        "from torchtext.data import Field, Dataset, Example\n",
        "import pandas as pd\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        \"\"\"\n",
        "        Create a dataset from a pandas dataframe of examples and Fields\n",
        "        Arguments:\n",
        "        examples pd.DataFrame: DataFrame of examples\n",
        "            fields {str: Field}: The Fields to use in this tuple. The\n",
        "            string is a field name, and the Field is the associated field.\n",
        "        filter_pred (callable or None): use only exanples for which\n",
        "            filter_pred(example) is true, or use all examples if None.\n",
        "            Default is None\n",
        "        \"\"\"\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "        # Unpack field tuples\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "                \n",
        "class SeriesExample(Example):\n",
        "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "\n",
        "        for key, field in fields.items():\n",
        "#             print(key)\n",
        "#             print(field)\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "        return ex"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWX0b8xSffKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(text, stop_ws=exl.stops, stemmer=None, str_output=False):\n",
        "\n",
        "    t = text.replace(\"-\", \" \").split(\" \")\n",
        "    t = [w.strip(string.punctuation) for w in t]\n",
        "\n",
        "    if stop_ws:\n",
        "        t = [w.lower() for w in t if w not in stop_ws]\n",
        "\n",
        "    if stemmer:\n",
        "        t = [stemmer.stem(w) for w in t]\n",
        "\n",
        "    if t == []:\n",
        "        return ['<blank>']\n",
        "    if str_output:\n",
        "        return ' '.join(t)\n",
        "    else:\n",
        "        return t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QaIVi9O1daj",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7ZraRaGffKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "TEXT = Field(tokenize=tokenizer, lower=True, include_lengths = True)\n",
        "LABEL = Field(sequential=False, unk_token=None, \n",
        "              dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tau-zbnLffKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = DataFrameDataset(train_sample, fields = {'label' : LABEL, \n",
        "                                                    'comment_text': TEXT})\n",
        "valid_ds = DataFrameDataset(val_samples, fields = {'label' : LABEL, \n",
        "                                                   'comment_text': TEXT})\n",
        "test_ds = DataFrameDataset(test_samples, fields = { 'label' : LABEL, \n",
        "                                                   'comment_text' : TEXT})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuzua6h9ffKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds.fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePOPjdgVffK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_ds.fields"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bneQDCt3ffKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train_ds, max_size=25000, \n",
        "#                  vectors = \"glove.840B.300d\",\n",
        "                 vectors = \"glove.6B.200d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_ds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiIC_VLaffK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
        "# print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8EMXlNfffKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TEXT.vocab.stoi['hope'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijUOcvskffKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(TEXT.vocab.itos[198])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMXfL_XYffKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Attribution: Adapted from https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95\n",
        "# import torch\n",
        "# from torchtext import data\n",
        "# import numpy as np\n",
        "\n",
        "# global max_text_in_batch\n",
        "\n",
        "# def batch_size_fn(new, count, sofar):\n",
        "#     \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "#     global max_text_in_batch\n",
        "#     if count == 1:\n",
        "#         max_text_in_batch = 0\n",
        "#     max_text_in_batch = max(max_text_in_batch, len(new.comment_text) + 2)\n",
        "#     text_elements = count * max_text_in_batch\n",
        "#     return text_elements\n",
        "\n",
        "\n",
        "# class EfficientIterator(data.Iterator):\n",
        "#     def create_batches(self):\n",
        "#         if self.train:\n",
        "#             def pool(d, random_shuffler):\n",
        "#                 for p in data.batch(d, self.batch_size * 100):\n",
        "#                     p_batch = data.batch(\n",
        "#                         sorted(p, key=self.sort_key),\n",
        "#                         self.batch_size, self.batch_size_fn)\n",
        "#                     for b in random_shuffler(list(p_batch)):\n",
        "#                         yield b\n",
        "#             self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "#         else:\n",
        "#             self.batches = []\n",
        "#             for b in data.batch(self.data(), self.batch_size,\n",
        "#                                           self.batch_size_fn):\n",
        "#                 self.batches.append(sorted(b, key=self.sort_key))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZD5ksVPffK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VOCAB_SIZE = len(TEXT.vocab)\n",
        "BATCH_SIZE = 1300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH5Zmse2xGNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "    (train_ds, valid_ds, test_ds), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.comment_text),\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4PAzMNUqs4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMsBFkLwffK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_iter = EfficientIterator(\n",
        "#     train_ds, batch_size=1300, \n",
        "#     device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "#     repeat=False, sort_key= lambda x: len(x.comment_text), batch_size_fn=batch_size_fn, \n",
        "#     train=True, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFCnCBlmffK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = next(iter(train_iter))\n",
        "print(batch.label)\n",
        "print(batch.label.size())\n",
        "print(batch.comment_text)\n",
        "print(batch.comment_text[0].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CkLGTRQz9Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_batch = next(iter(val_iter))\n",
        "print(val_batch.label)\n",
        "print(val_batch.label.size())\n",
        "print(val_batch.comment_text)\n",
        "print(val_batch.comment_text[0].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXnBVtcO0Ss0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_batch = next(iter(test_iter))\n",
        "print(test_batch.label)\n",
        "print(test_batch.label.size())\n",
        "print(test_batch.comment_text)\n",
        "print(test_batch.comment_text[0].size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOBIYlqqffLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# val_iter = EfficientIterator(\n",
        "#     valid_ds, batch_size=1300, \n",
        "#     device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
        "#     repeat=False, sort_key= lambda x: len(x.comment_text), batch_size_fn=batch_size_fn, \n",
        "#     train=True, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljuHEYXjz9kr",
        "colab_type": "text"
      },
      "source": [
        "### Integrate Batch Loader into Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m7TpXvUffLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, pad_idx, batch_size=1,\n",
        "                 embed_dim=6, weight_decay=0, optimizer_fcn='Adam',\n",
        "                 learning_rate=1e-3, num_layers=2, dropout=0.05, \n",
        "                 num_classes=1, bidirectional=True):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        nn.Module.__init__(self)\n",
        "#         TextData.__init__(self, X_data)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.batch_size = batch_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dropout = dropout\n",
        "        self.output_dim = num_classes\n",
        "        self.loss_fcn = nn.NLLLoss()\n",
        "        self.weight_decay = weight_decay\n",
        "        self.learning_rate = learning_rate\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        # Layer 1: Embedding Layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # Layer 2: LSTM Layer\n",
        "        self.lstm = nn.LSTM(input_size = self.embed_dim, \n",
        "                            hidden_size = self.hidden_dim,\n",
        "                            num_layers = self.num_layers, \n",
        "                            dropout = self.dropout,\n",
        "                            bidirectional=bidirectional) \n",
        "#                             batch_first=True)\n",
        "\n",
        "        # Layer 3 (Output Layer): Linear\n",
        "        self.linear = nn.Linear(self.hidden_dim * 2, self.output_dim)\n",
        "    \n",
        "        # Dropout Layer  \n",
        "        self.dropout_layer = nn.Dropout(self.dropout)\n",
        "\n",
        "        # define optimizer\n",
        "        if optimizer_fcn == 'Adam':\n",
        "            self.optimizer = optim.Adam(params=self.parameters(),\n",
        "                                                 weight_decay=self.weight_decay,\n",
        "                                                 lr=self.learning_rate)\n",
        "        elif optimizer_fcn == 'RMSprop':\n",
        "            self.optimizer = optim.RMSprop(params=self.parameters(),\n",
        "                                                 weight_decay=self.weight_decay,\n",
        "                                                 lr=self.learning_rate)\n",
        "        elif optimizer_fcn == 'SDG':\n",
        "            self.optimizer = optim.SGD(params=self.parameters(),\n",
        "                                                 weight_decay=self.weight_decay,\n",
        "                                                 lr=self.learning_rate)\n",
        "            \n",
        "\n",
        "    def forward(self, input_seq, input_len):\n",
        "        # input_seq shape is [batch_max_input_len, batch_size]\n",
        "        embed_out = self.embedding(input_seq)\n",
        "        embed_out = self.dropout_layer(embed_out)\n",
        "\n",
        "        # embed_out shape is [batch_max_input_len, batch_size, embed_dim]\n",
        "\n",
        "        \n",
        "        # tell LSTM layer to focus on non-padded elements by packing embeddings\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embed_out, input_len)\n",
        "        \n",
        "        packed_lstm_out, (hn, cn) = self.lstm(packed_embedded)\n",
        "        # hn shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
        "        \n",
        "        # unpack sequences into tensors if need them\n",
        "        # out, out_len = nn.utils.rnn.pad_packed_sequence(packed_lstm_out)\n",
        "        # out shape is [batch_max_input_len, batch_size, hidden_dim * num directions]\n",
        "        \n",
        "        #concat final forward and backward hidden layers to apply any dropout\n",
        "        hn = self.dropout_layer(torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1))\n",
        "        # hn shape is [batch size, hid dim * num directions]\n",
        "        \n",
        "#         out = F.log_softmax(self.linear(hn.squeeze(0)), dim=self.output_dim) \n",
        "        out = F.log_softmax(self.linear(hn), dim=1)\n",
        "        # out shape is [batch_size, outpu_dim]\n",
        "        \n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJW8CophffLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_clf(model, iterator):\n",
        "    running_loss = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_nontarget_recall = 0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "#         if len(batch) != iterator.batch_size:\n",
        "#             print(f\"Skipping small batch of length {len(batch)}...\")\n",
        "#             continue\n",
        "          \n",
        "        model.optimizer.zero_grad()\n",
        "\n",
        "        seq, seq_lengths = batch.comment_text\n",
        "        y = batch.label.long()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            seq = seq.cuda() \n",
        "            y = y.cuda()\n",
        "            seq_lengths = seq_lengths.cuda()\n",
        "\n",
        "        preds = model.forward(seq, seq_lengths)\n",
        "\n",
        "        loss = model.loss_fcn(input=preds.squeeze(1), \n",
        "                                     target=y)\n",
        "\n",
        "        y_true = [single_tensor.item() for single_tensor in y]\n",
        "\n",
        "        indices = torch.argmax(preds, 1)\n",
        "        y_pred = [single_tensor.item() for single_tensor in indices.long()]\n",
        "\n",
        "        recall_toxic = recall_score(y_true, y_pred, pos_label=1)\n",
        "        recall_nontoxic = recall_score(y_true, y_pred, pos_label=0)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        epoch_recall += recall_toxic.item()\n",
        "        epoch_nontarget_recall += recall_nontoxic.item()\n",
        "\n",
        "        model.optimizer.step()\n",
        "        \n",
        "    last_mean_error = np.mean(np.abs(loss.item()))\n",
        "    return running_loss / len(iterator), epoch_recall / len(iterator),  epoch_nontarget_recall / len(iterator), last_mean_error\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHo1z3ARffLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_clf(model, iterator):\n",
        "    \n",
        "    running_loss = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_nontarget_recall = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            seq, seq_lengths = batch.comment_text\n",
        "            \n",
        "            y = batch.label.long()\n",
        "            \n",
        "            predictions = model.forward(seq, seq_lengths)\n",
        "            \n",
        "            loss = model.loss_fcn(predictions.squeeze(1), y)\n",
        "            \n",
        "            val_true = [single_tensor.item() for single_tensor in y]\n",
        "\n",
        "            indices = torch.argmax(predictions, 1)\n",
        "            val_pred = [single_tensor.item() for single_tensor in indices]\n",
        "\n",
        "            f1_toxic = f1_score(val_true, val_pred, pos_label=1)\n",
        "            f1_nontoxic = f1_score(val_true, val_pred, pos_label=0)\n",
        "        \n",
        "            recall_toxic = recall_score(val_true, val_pred, pos_label=1)\n",
        "            recall_nontoxic = recall_score(val_true, val_pred, pos_label=0)\n",
        "            \n",
        "            precision_toxic = precision_score(val_true, val_pred, pos_label=1)\n",
        "            precision_nontoxic = precision_score(val_true, val_pred, pos_label=0)\n",
        "            \n",
        "            accuracy = accuracy_score(val_true, val_pred)\n",
        "            \n",
        "            try:\n",
        "                auc_roc = roc_auc_score(val_true, val_pred)\n",
        "            except:\n",
        "                auc_roc = -1\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            epoch_recall += recall_toxic.item()\n",
        "            epoch_nontarget_recall += recall_nontoxic.item()\n",
        "  \n",
        "            metrics = {\n",
        "                \"target\": {\"f1\": round(f1_toxic * 100, 5),\n",
        "                           \"precision\": round(precision_toxic * 100, 5),\n",
        "                           \"recall\": round(recall_toxic * 100, 5)\n",
        "                          },\n",
        "                \"nontarget\": {\"f1\": round(f1_nontoxic * 100, 5),\n",
        "                           \"precision\": round(precision_nontoxic * 100, 5),\n",
        "                           \"recall\": round(recall_nontoxic * 100)\n",
        "                          },\n",
        "                \"overall\": {\"accuracy\": round(accuracy * 100, 5),\n",
        "                           \"auc_roc\": round(auc_roc * 100, 5)}\n",
        "            }\n",
        "   \n",
        "        \n",
        "    return running_loss / len(iterator), epoch_recall / len(iterator),  epoch_nontarget_recall / len(iterator), metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZu4Ql8yffLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLaTlirWCuVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loop_epochs(model, train_iter, val_iter, num_epochs, outfile=None, \n",
        "                model_prefix=None):  \n",
        "    loop_start = time.time()\n",
        "    if outfile:\n",
        "        f = open(outfile, \"a\")\n",
        "    else:\n",
        "        f = None\n",
        "      \n",
        "    # best_valid_loss = float('inf')\n",
        "    last_mean_error = -math.inf\n",
        "    best_target_recall = -1\n",
        "    best_nontarget_recall = -1\n",
        "\n",
        "    thresholds= {\n",
        "            5000: 1000,\n",
        "            1000: 100,\n",
        "            100: 10,\n",
        "            20: 5,\n",
        "            10: 3\n",
        "        }\n",
        "    metric_results = dict()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"Epoch {epoch}:\\nStarting training...\", file=f)\n",
        "        train_loss, target_recall, nontarget_recall, epoch_mean_error = train_clf(model, train_iter)\n",
        "\n",
        "        end_train = time.time()\n",
        "        train_mins, train_secs = epoch_time(start_time, end_train)\n",
        "\n",
        "        print(f\"Starting evaluation...({train_mins:02}:{train_secs:02} elapsed)\", file=f)\n",
        "        valid_loss, val_target_recall, val_nontarget_recall, metrics = evaluate_clf(model, val_iter)\n",
        "\n",
        "        end_eval = time.time()\n",
        "        eval_mins, eval_secs = epoch_time(end_train, end_eval)\n",
        "        print(f\"Finished evaluation...({eval_mins:02}:{eval_secs:02} elapsed)\", file=f)\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_eval)\n",
        "\n",
        "        metric_results[epoch] = metrics\n",
        "\n",
        "        if val_target_recall > best_target_recall:\n",
        "            best_target_recall = val_target_recall\n",
        "            savestate_file = f'{model_prefix}_epoch{epoch}_target.pt'\n",
        "            torch.save(model.state_dict(), savestate_file)\n",
        "            \n",
        "        if val_nontarget_recall > best_nontarget_recall:\n",
        "            best_nontarget_recall = val_nontarget_recall\n",
        "            savestate_file = f'{model_prefix}_epoch{epoch}_nontarget.pt'\n",
        "            torch.save(model.state_dict(), savestate_file)     \n",
        "\n",
        "        print(f'Epoch: {epoch+1:02} | Total Epoch Time: {epoch_mins}m {epoch_secs}s', file=f)\n",
        "        print(f'\\tNegative Log Linear Loss:  Loss: {train_loss:.6f} | Target Recall: {target_recall*100:.2f}% | Nontarget Recall: {nontarget_recall*100:.2f}%', file=f)\n",
        "        print(f'\\tVal. Loss: {valid_loss:.6f} | Target Recall: {val_target_recall*100:.2f}% | Nontarget Recall: {val_nontarget_recall*100:.2f}%', file=f)\n",
        "        \n",
        "        \n",
        "    loop_mins, loop_secs = epoch_time(loop_start, end_eval)\n",
        "    print(f\"Total Time: {loop_mins:02}:{loop_secs:02} elapsed\", file=f)\n",
        "    print('', file=f)\n",
        "    print(metric_results, file=f)\n",
        "    print('', file=f)\n",
        "    print('', file=f)\n",
        "    f.close()\n",
        "    if outfile:\n",
        "        send_to_bucket(filename=outfile, directory=\"model_results\")\n",
        "    \n",
        "    send_to_bucket(filename=savestate_file, directory=\"model_states\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcnufZwcNRJ3",
        "colab_type": "text"
      },
      "source": [
        "### Test Model Functions on 20% of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwGzu7VtffLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 6\n",
        "hist_lstm = np.zeros(NUM_EPOCHS)\n",
        "model_name=\"20_pct_dataset\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrnl5iguffLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model = LSTMModel(vocab_size = VOCAB_SIZE, hidden_dim=100, num_layers=1, \n",
        "#                        embed_dim=300, \n",
        "                       embed_dim=200, \n",
        "                       batch_size=train_iter.batch_size,\n",
        "                       dropout=0, num_classes=2, \n",
        "                       pad_idx=TEXT.vocab.stoi[TEXT.pad_token])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF-cNIvwffLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm6oe4bPffLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    lstm_model = lstm_model.cuda()\n",
        "    lstm_model.loss_fcn = lstm_model.loss_fcn.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOGOvGmP8qEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy_Wg_u28rK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm_model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL0gKpd09D96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let model learn from unknown token, while leaving padding as zeros\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "lstm_model.embedding.weight.data[UNK_IDX] = torch.zeros(lstm_model.embed_dim)\n",
        "lstm_model.embedding.weight.data[lstm_model.pad_idx] = torch.zeros(lstm_model.embed_dim)\n",
        "\n",
        "print(lstm_model.embedding.weight.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv2NPuqyEFhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loop_epochs(lstm_model, train_iter, val_iter, num_epochs=NUM_EPOCHS, \n",
        "            outfile='20pct_imbalanced.txt', model_prefix='20pct_imbalanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS13y5MAOL9v",
        "colab_type": "text"
      },
      "source": [
        "## Modeling on Rebalanced Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjPbkKUsy-wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyZbb9NmhRvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initiate_model(train_df, val_df, batch_size=BATCH_SIZE):\n",
        "    \n",
        "    TEXT = Field(tokenize=tokenizer, lower=True, include_lengths = True)\n",
        "    LABEL = Field(sequential=False, unk_token=None, \n",
        "              dtype=torch.float)\n",
        "    \n",
        "    train_ds = DataFrameDataset(train_df, fields = {'label' : LABEL, \n",
        "                                                        'comment_text': TEXT})\n",
        "    valid_ds = DataFrameDataset(val_df, fields = {'label' : LABEL, \n",
        "                                                       'comment_text': TEXT})\n",
        "#     test_ds = DataFrameDataset(test_df, fields = { 'label' : LABEL, \n",
        "#                                                        'comment_text' : TEXT})\n",
        "    print(\"Torchtext Datasets created...\")\n",
        "    TEXT.build_vocab(train_ds, max_size=25000, \n",
        "#                  vectors = \"glove.840B.300d\",\n",
        "                 vectors = \"glove.6B.200d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "    LABEL.build_vocab(train_ds)\n",
        "    \n",
        "    print(\"Torchtext vocabulary cerated...\")\n",
        "    \n",
        "    VOCAB_SIZE = len(TEXT.vocab)\n",
        "    \n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    train_iter, val_iter = BucketIterator.splits(\n",
        "    (train_ds, valid_ds), \n",
        "    batch_size = batch_size,\n",
        "    sort_key=lambda x: len(x.comment_text),\n",
        "    sort_within_batch = True,\n",
        "    device = device)\n",
        "    \n",
        "    print(\"Torchtext iterators cerated...\")\n",
        "    \n",
        "    lstm_model = LSTMModel(vocab_size = VOCAB_SIZE, hidden_dim=100, num_layers=1, \n",
        "                           embed_dim=200, \n",
        "                           batch_size=train_iter.batch_size,\n",
        "                           dropout=0, num_classes=2, \n",
        "                           pad_idx=TEXT.vocab.stoi[TEXT.pad_token])\n",
        "    \n",
        "    print(\"Model created...\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        lstm_model = lstm_model.cuda()\n",
        "        lstm_model.loss_fcn = lstm_model.loss_fcn.cuda()\n",
        "        \n",
        "    pretrained_embeddings = TEXT.vocab.vectors\n",
        "    lstm_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "    lstm_model.embedding.weight.data[UNK_IDX] = torch.zeros(lstm_model.embed_dim)\n",
        "    lstm_model.embedding.weight.data[lstm_model.pad_idx] = torch.zeros(lstm_model.embed_dim)\n",
        "    print(\"Pre-trained embeddings applied...\")\n",
        "    \n",
        "    return lstm_model, train_iter, val_iter\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CorHzdnN00YL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rebalance_dict = {0: 35, 1: 50, 2: 60, 3: 65, 4:75, 5: 'random'}\n",
        "data_proportions = [0.2, 0.4, 0.6, 0.8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o8nY8D9OW4n",
        "colab_type": "text"
      },
      "source": [
        "### Rebalanced Toxicity on 20% of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnm3k6FmWUCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "prepared_35, prepared_50, prepared_60, prepared_65, prepared_75, random_df = exl.rebalance_data(train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs_d52XG14nA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_ratio=0.2\n",
        "\n",
        "for i, df in enumerate([prepared_35, prepared_50, prepared_60, prepared_65, prepared_75, random_df]):\n",
        "    model_name=f'20pct_model_{rebalance_dict[i]}toxic'\n",
        "    print(f'Model: {model_name}')\n",
        "    val_sample_df = val_set.sample( n=math.ceil(len(df)*test_ratio), random_state=1008 )\n",
        "    \n",
        "    print(f\"Validation sample_created...\")\n",
        "    \n",
        "    lstm_model, train_iter, val_iter = initiate_model(df, val_sample_df, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    print(f\"Beginning training and evaluation loops...\")\n",
        "\n",
        "    loop_epochs(lstm_model, train_iter, val_iter, num_epochs=NUM_EPOCHS, \n",
        "            outfile=f'{model_name}.txt', model_prefix=model_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJV8iFbOhaY",
        "colab_type": "text"
      },
      "source": [
        "### Rebalanced Toxicity on 40%, 60%, 80% of Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggXHXrwZ4RI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 6\n",
        "hist_lstm = np.zeros(NUM_EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjmFzYvusQ97",
        "colab_type": "code",
        "outputId": "8ff0e2a2-9381-4b2e-bffe-4885816443e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "for proportion in data_proportions[3:]:\n",
        "    train_set, val_set, test_set = exl.get_samples(train_full, proportion=proportion, \n",
        "                                                  train_test_ratio=(1-test_ratio))\n",
        "    \n",
        "    pickle_to_gcs(test_set, f'{proportion*100:.0f}pct_test_set.pkl', \n",
        "                  directory=\"test_sets\")\n",
        "        \n",
        "    rebalanced_dfs = exl.rebalance_data(train_set)\n",
        "    \n",
        "    \n",
        "    \n",
        "    for i, df in enumerate(rebalanced_dfs):\n",
        "        if (rebalance_dict[i] in {35}) and (proportion == 0.8):\n",
        "            continue\n",
        "   \n",
        "        model_name=f'{proportion*100:.0f}pct_model_{rebalance_dict[i]}toxic'\n",
        "        \n",
        "        print(f'Model: {model_name}')\n",
        "        \n",
        "        pickle_to_gcs(df, f'{model_name}_train.pkl', directory=\"train_sets\")\n",
        "        \n",
        "        try:\n",
        "            val_sample_df = val_set.sample( n=math.ceil(len(df)*test_ratio), \n",
        "                                           random_state=1008 )\n",
        "        except:\n",
        "            val_sample_df = val_set.sample( n=math.ceil(len(df)*test_ratio), \n",
        "                                           random_state=1008, replace = True )\n",
        "            \n",
        "        pickle_to_gcs(val_sample_df, f'{model_name}_val.pkl', directory=\"val_sets\")\n",
        "\n",
        "        print(f\"Validation sample_created...\")\n",
        "\n",
        "        lstm_model, train_iter, val_iter = initiate_model(df, val_sample_df, \n",
        "                                                          batch_size=BATCH_SIZE)\n",
        "\n",
        "        print(f\"Beginning training and evaluation loops...\")\n",
        "\n",
        "        loop_epochs(lstm_model, train_iter, val_iter, num_epochs=NUM_EPOCHS, \n",
        "                outfile=f'{model_name}.txt', model_prefix=model_name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retrieving training samples at proportion 0.8\n",
            "Rebalance Ratio: 0.35, 404283 toxic samples out of 1155095\n",
            "Rebalance Ratio: 0.5, 577547 toxic samples out of 1155095\n",
            "Rebalance Ratio: 0.6, 693057 toxic samples out of 1155095\n",
            "Rebalance Ratio: 0.65, 750811 toxic samples out of 1155095\n",
            "Rebalance Ratio: 0.75, 866321 toxic samples out of 1155095\n",
            "Rebalanced dfs created...\n",
            "Model: 80pct_model_35toxic\n",
            "Validation sample_created...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/80pct_model_35toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "Copying file:///content/80pct_model_35toxic_epoch5_nontarget.pt [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Model: 80pct_model_50toxic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odFz6WpJ6sle",
        "colab_type": "text"
      },
      "source": [
        "### Large Dataset Troubhleshooting\n",
        "80% datasets are crashing RAM, so create dfs, pickle, and run individually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT9a8CkQlyvq",
        "colab_type": "code",
        "outputId": "e7edcee3-31de-48e0-8fea-8d32b5259333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "\n",
        "train_set, val_set, test_set = exl.get_samples(train_full, proportion=0.8, \n",
        "                                              train_test_ratio=(1-test_ratio))\n",
        "\n",
        "pickle_to_gcs(test_set, f'80pct_test_set_past35.pkl', \n",
        "              directory=\"test_sets\")\n",
        "\n",
        "rebalanced_dfs = exl.rebalance_data(train_set)\n",
        "\n",
        "for i, df in enumerate(rebalanced_dfs):\n",
        "    if i == 0:\n",
        "        continue\n",
        "    model_name = f'80pct_model_{rebalance_dict[i]}toxic'\n",
        "    pickle_to_gcs(df, f'{model_name}_train.pkl', directory=\"train_sets\")\n",
        "    \n",
        "    try:\n",
        "        val_sample_df = val_set.sample( n=math.ceil(len(df)*test_ratio), \n",
        "                                           random_state=1008 )\n",
        "        pickle_to_gcs(val_sample_df, f'{model_name}_val.pkl', directory=\"val_sets\")\n",
        "        \n",
        "    except:\n",
        "        val_sample_df = val_set.sample( n=math.ceil(len(df)*test_ratio), \n",
        "                                       random_state=1008, replace = True )\n",
        "        \n",
        "        pickle_to_gcs(val_sample_df, f'{model_name}_val.pkl', directory=\"val_sets\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Retrieving training samples at proportion 0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gcsfs/core.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=85, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 57806), raddr=('74.125.142.95', 443)>\n",
            "  self._singleton[0] = self\n",
            "/usr/local/lib/python3.6/dist-packages/gcsfs/core.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=84, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 34352), raddr=('74.125.20.84', 443)>\n",
            "  self._singleton[0] = self\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Rebalance Ratio: 0.35, 404273 toxic samples out of 1155068\n",
            "Rebalance Ratio: 0.5, 577534 toxic samples out of 1155068\n",
            "Rebalance Ratio: 0.6, 693040 toxic samples out of 1155068\n",
            "Rebalance Ratio: 0.65, 750794 toxic samples out of 1155068\n",
            "Rebalance Ratio: 0.75, 866301 toxic samples out of 1155068\n",
            "Rebalanced dfs created...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elas01KrifZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_given_dfs(train_pkl, val_pkl, gcs=False):\n",
        "    if gcs:   \n",
        "        model_name = train_pkl.replace('_train.pkl', '')\n",
        "        train_df = load_pickle_from_gcs(train_pkl, directory=\"train_sets\") \n",
        "        val_df = load_pickle_from_gcs(val_pkl, directory=\"val_sets\")  \n",
        "    \n",
        "    else:\n",
        "        model_name = train_pkl.replace('_train.pkl', '')\n",
        "        train_df = pd.read_pickle(train_pkl)\n",
        "        val_df = pd.read_pickle(val_pkl)\n",
        "    \n",
        "    print(\"Initiating model and iterators...\")\n",
        "\n",
        "    lstm_model, train_iter, val_iter = initiate_model(train_df, val_df, \n",
        "                                                      batch_size=BATCH_SIZE)\n",
        "\n",
        "    print(f\"Beginning training and evaluation loops...\")\n",
        "\n",
        "    loop_epochs(lstm_model, train_iter, val_iter, num_epochs=NUM_EPOCHS, \n",
        "            outfile=f'{model_name}.txt', model_prefix=model_name)\n",
        "    \n",
        "    print(\"Done!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoLFU8oL70jv",
        "colab_type": "code",
        "outputId": "953bfccd-3770-4515-fa12-c06938c57edd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "run_given_dfs('80pct_model_50toxic_train.pkl', '80pct_model_50toxic_val.pkl')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiated model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/80pct_model_50toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "Copying file:///content/80pct_model_50toxic_epoch4_nontarget.pt [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WovaCes-HAh",
        "colab_type": "code",
        "outputId": "761a151a-a5cd-45c3-ffe2-c4c0045f88a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "run_given_dfs('40pct_model_35toxic_train.pkl', '40pct_model_35toxic_val.pkl', gcs=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [00:32, 26.2MB/s]                           \n",
            "100%|█████████▉| 398557/400000 [00:27<00:00, 15233.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398557/400000 [00:40<00:00, 15233.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/40pct_model_35toxic.txt [Content-Type=text/plain]...\n",
            "/ [0 files][    0.0 B/  3.0 KiB]                                                \r/ [1 files][  3.0 KiB/  3.0 KiB]                                                \r\n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/40pct_model_35toxic_epoch4_nontarget.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_8_Vj_m-HJN",
        "colab_type": "code",
        "outputId": "6e4b9e93-38ce-40c1-a5dc-9fc9b997ab4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('40pct_model_65toxic_train.pkl', '40pct_model_65toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/40pct_model_65toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "Copying file:///content/40pct_model_65toxic_epoch4_target.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfmz7zgO-AB5",
        "colab_type": "code",
        "outputId": "db261647-1ea6-4b3b-b477-769d3ac0adc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('40pct_model_75toxic_train.pkl', '40pct_model_75toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/40pct_model_75toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/40pct_model_75toxic_epoch3_target.pt [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJFj_TMELhCO",
        "colab_type": "code",
        "outputId": "ecd8322f-f75a-4e8b-8c6e-d8ab428be372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('60pct_model_35toxic_train.pkl', '60pct_model_35toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/60pct_model_35toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/60pct_model_35toxic_epoch5_nontarget.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnB6dcCy7_5I",
        "colab_type": "code",
        "outputId": "22512949-1190-42e7-da42-58f2779d49d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('80pct_model_60toxic_train.pkl', '80pct_model_60toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/80pct_model_60toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/80pct_model_60toxic_epoch4_target.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeAL5Fqn8Eo9",
        "colab_type": "code",
        "outputId": "60e1f0b0-09f7-47e0-ece8-a402999d322a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('80pct_model_65toxic_train.pkl', '80pct_model_65toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/80pct_model_65toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/80pct_model_65toxic_epoch4_target.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7AuSUzI8EyZ",
        "colab_type": "code",
        "outputId": "9429bd71-18bd-4977-892d-4e1ec609f5e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "run_given_dfs('80pct_model_75toxic_train.pkl', '80pct_model_75toxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n",
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n",
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n",
            "Copying file:///content/80pct_model_75toxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  3.0 KiB/  3.0 KiB]                                                \n",
            "Operation completed over 1 objects/3.0 KiB.                                      \n",
            "Copying file:///content/80pct_model_75toxic_epoch5_target.pt [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 20.0 MiB/ 20.0 MiB]                                                \n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI3OlZc68JJm",
        "colab_type": "code",
        "outputId": "1b565232-c8b0-4fe8-bc98-3a88f6b1d9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "run_given_dfs('80pct_model_randomtoxic_train.pkl', '80pct_model_randomtoxic_val.pkl', gcs=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gcsfs/core.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=67, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 43386), raddr=('216.58.197.170', 443)>\n",
            "  self._singleton[0] = self\n",
            "/usr/local/lib/python3.6/dist-packages/gcsfs/core.py:304: ResourceWarning: unclosed <ssl.SSLSocket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.28.0.2', 47590), raddr=('172.217.31.173', 443)>\n",
            "  self._singleton[0] = self\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initiating model and iterators...\n",
            "Torchtext Datasets created...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [16:46, 857kB/s]                            \n",
            "100%|█████████▉| 398622/400000 [00:27<00:00, 14588.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Torchtext vocabulary cerated...\n",
            "Torchtext iterators cerated...\n",
            "Model created...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398622/400000 [00:40<00:00, 14588.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Pre-trained embeddings applied...\n",
            "Beginning training and evaluation loops...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Copying file:///content/80pct_model_randomtoxic.txt [Content-Type=text/plain]...\n",
            "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
            "Operation completed over 1 objects/2.9 KiB.                                      \n",
            "Copying file:///content/80pct_model_randomtoxic_epoch3_target.pt [Content-Type=application/octet-stream]...\n",
            "-\n",
            "Operation completed over 1 objects/20.0 MiB.                                     \n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}