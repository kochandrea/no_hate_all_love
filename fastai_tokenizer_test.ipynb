{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['label'] = train.target.apply(lambda x: assign_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv(\"s3://advancedml-koch-mathur-hinkson/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_preprocessed = pd.read_pickle('mini_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini_preprocessed.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_label = lambda x: 0 if x < 0.5 else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_preprocessed['label'] = mini_preprocessed.target.apply(lambda x: assign_label(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOXIC_LABEL = 'toxic'\n",
    "NOT_TOXIC_LABEL = 'not_toxic'  \n",
    "VOCAB_SIZE = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class TextData:\n",
    "    def __init__(self, df, text_col='cleaned_no_stem'):\n",
    "        # pull relevant data from df\n",
    "        self.preprocessed_text = [word_list for word_list in df[text_col] ]\n",
    "        \n",
    "        # gather vocabulary corpus to store all words in training data\n",
    "        self.vocab = Counter([word for comment in self.preprocessed_text \n",
    "                              for word in comment]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {TOXIC_LABEL: 1, NOT_TOXIC_LABEL: 0}\n",
    "        self.idx_to_label = [NOT_TOXIC_LABEL, TOXIC_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    Get batched data for training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx # dictionary {str: int}\n",
    "        self.label_to_idx = {TOXIC_LABEL: 1, NOT_TOXIC_LABEL: 0} # dictionary\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.zeros(self.vocab_size)\n",
    "        \n",
    "        item = torch.from_numpy(item)\n",
    "        # use both the document and label for training/ tuning (have comment and label)\n",
    "        if len(self.data[idx]) == 2: \n",
    "            for word in word_tokenize(self.data[idx][0]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            label = self.label_to_idx[self.data[idx][1]]\n",
    "            return item, label\n",
    "        \n",
    "        else: # use the document without label for testing (have only comment)\n",
    "            for word in word_tokenize(self.data[idx]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            return item\n",
    "        \n",
    "        \n",
    "     ##  Override single items' getter\n",
    "    def __getitem__(self, idx):\n",
    "        if idx + self.seq_length > self.__len__():\n",
    "            if self.transforms is not None:\n",
    "                item = torch.zeros(self.seq_length, self.dataset[0].__len__())\n",
    "                item[:self.__len__()-idx] = self.transforms(self.dataset[idx:])\n",
    "                return item, item\n",
    "            else:\n",
    "                item = []\n",
    "                item[:self.__len__()-idx] = self.dataset[idx:]\n",
    "                return item, item\n",
    "        else:\n",
    "            if self.transforms is not None:\n",
    "                return self.transforms(self.dataset[idx:idx+self.seq_length]), self.transforms(self.dataset[idx:idx+self.seq_length])\n",
    "            else:\n",
    "                return self.dataset[idx:idx+self.seq_length], self.dataset[idx:idx+self.seq_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_data = [tuple(x) for x in mini_preprocessed[['cleaned_no_stem', 'label']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [0, 1]\n",
    "def get_texts(df, text_field=\"comment_text\", label_field=\"label\"):\n",
    "    texts,labels = [],[]\n",
    "    for idx,label in enumerate(CLASSES):\n",
    "         for i in df.index:\n",
    "            text = df.loc[i, text_field]\n",
    "            label = df.loc[i, label_field]\n",
    "#             print(text)\n",
    "#             print(label)\n",
    "#             print(fields)\n",
    "#             examples.append(Example.fromlist([text, label], fields))\n",
    "#         for fname in (path/label).glob('*.*'):\n",
    "            texts.append(text)\n",
    "            labels.append(label)\n",
    "    return np.array(texts),np.array(labels), [label_field, text_field]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train/ Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 60) (8000,)\n",
      "(2000, 60) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_data, X_test_data, y_train_data, y_test_data = train_test_split(mini_preprocessed.drop('label', axis=1), \n",
    "                                                                        mini_preprocessed['label'], \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 4812)\n",
    "\n",
    "print(X_train_data.shape, y_train_data.shape)\n",
    "print(X_test_data.shape, y_test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 4000)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_texts,trn_labels, col_names = get_texts(mini_preprocessed.loc[X_train_data.index])\n",
    "val_texts,val_labels, _ = get_texts(mini_preprocessed.loc[X_test_data.index])\n",
    "len(trn_texts),len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['labels','text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle indices in train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_texts))\n",
    "val_idx = np.random.permutation(len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts = trn_texts[trn_idx]\n",
    "val_texts = val_texts[val_idx]\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = val_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, \n",
    "#                       columns=col_names)\n",
    "# df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels},\n",
    "#                       columns=col_names)\n",
    "# df_trn[df_trn['labels']!=2].to_csv(data/'train.csv',\n",
    "#                                    header=False, index=False)\n",
    "# df_val.to_csv(CLAS_PATH/'test.csv', header=False, index=False)\n",
    "with open('data/classes.txt', 'w') as f:\n",
    "    f.writelines(f'{o}\\n' for o in CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0\\n', '1\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('data/classes.txt', 'r') as r:\n",
    "    print(r.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 2000)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_texts,val_texts = train_test_split(\n",
    "    np.concatenate([trn_texts,val_texts]), test_size=0.1)\n",
    "len(trn_texts), len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize=24000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "re1 = re.compile(r'  +')\n",
    "\n",
    "def fixup(x):\n",
    "    '''\n",
    "    H/T to the fastai.text team!\n",
    "    '''\n",
    "    x = x.replace('#39;', \"'\")\\\n",
    "        .replace('amp;', '&')\\\n",
    "        .replace('#146;', \"'\")\\\n",
    "        .replace('nbsp;', ' ')\\\n",
    "        .replace('#36;', '$')\\\n",
    "        .replace('\\\\n', \"\\n\")\\\n",
    "        .replace('quot;', \"'\")\\\n",
    "        .replace('<br />', \"\\n\")\\\n",
    "        .replace('\\\\\"', '\"')\\\n",
    "        .replace('<unk>', 'u_n')\\\n",
    "        .replace(' @.@ ', '.')\\\n",
    "        .replace(' @-@ ', '-')\\\n",
    "        .replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = !getconf _NPROCESSORS_ONLN\n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(df, n_lbls=1, fieldflag):\n",
    "    '''\n",
    "    H/T to the fastai.text team!\n",
    "    '''\n",
    "    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n",
    "    for i in range(n_lbls+1, len(df.columns)): \n",
    "        texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    \n",
    "    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, stop_ws=True, stemmer='Porter', str_output=False):\n",
    "    \n",
    "    # intialize stemmers\n",
    "    from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "    ps = PorterStemmer() \n",
    "    ls = LancasterStemmer()\n",
    "    \n",
    "    t = text.replace(\"-\", \" \").split(\" \")\n",
    "    t = [w.strip(string.punctuation) for w in t]\n",
    "    \n",
    "    if stop_ws:\n",
    "        from nltk.corpus import stopwords\n",
    "        # define stopwords\n",
    "        stops = set(stopwords.words('english'))\n",
    "        stops.add('')\n",
    "\n",
    "        approved_stop_words = {\"not\", \"get\", \"against\", \"haven\", \"haven't\",\"aren't\", \n",
    "                               \"aren\", \"should\", \"shouldn\", \"shouldn't\", \"themselves\", \n",
    "                               \"them\", \"under\", \"over\", 'won', \"won't\", \"wouldn'\", \n",
    "                               \"wouldn't\"}\n",
    "        stops = stops - approved_stop_words\n",
    "        t = [w.lower() for w in t if w not in stop_ws]\n",
    "    \n",
    "    if stemmer:\n",
    "        if stemmer == 'Porter':\n",
    "            t = [ps.stem(w) for w in t]\n",
    "        elif stemmer == 'Lancaster':\n",
    "            t = [ls.stem(w) for w in t]\n",
    "    \n",
    "    if str_output:\n",
    "        return ' '.join(t)\n",
    "    else:\n",
    "        return t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".aml",
   "language": "python",
   "name": ".aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
